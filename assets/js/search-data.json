{
  
    
        "post0": {
            "title": "2.7 NLP Basics Practice",
            "content": "For this assessment we&#39;ll be using the short story An Occurrence at Owl Creek Bridge by Ambrose Bierce (1890). The story is in the public domain; the text file was obtained from Project Gutenberg. . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . 1. Create a Doc object from the file owlcreek.txt . HINT:Use with open(&#39;../TextFiles/owlcreek.txt&#39;) as f: . . doc[:36] . AN OCCURRENCE AT OWL CREEK BRIDGE by Ambrose Bierce I A man stood upon a railroad bridge in northern Alabama, looking down into the swift water twenty feet below. . Solution . with open(&#39;data_files/owlcreek.txt&#39;) as f: doc = nlp(f.read()) print(doc[:36]) . AN OCCURRENCE AT OWL CREEK BRIDGE by Ambrose Bierce I A man stood upon a railroad bridge in northern Alabama, looking down into the swift water twenty feet below. . 2. How many tokens are contained in the file? . Solution . len(doc) . 4835 . 3. How many sentences are contained in the file?HINT: You&#39;ll want to build a list first! . . 211 . Solution . sentences = [] for sent in doc.sents: sentences.append(sent) len(sentences) . 205 . sents= [sent for sent in doc.sents] len(sents) . 205 . 4. Print the second sentence in the document HINT: Indexing starts at zero, and the title counts as the first sentence. . Solution . sentences[0].text . &#39;AN OCCURRENCE AT OWL CREEK BRIDGE n nby Ambrose Bierce n nI n nA man stood upon a railroad bridge in northern Alabama, looking down ninto the swift water twenty feet below.&#39; . 5. For each token in the sentence above, print its text, POS tag, dep tag and lemma CHALLENGE: Have values line up in columns in the print output. . . A DET det a man NOUN nsubj man stood VERB ROOT stand upon ADP prep upon a DET det a railroad NOUN compound railroad bridge NOUN pobj bridge in ADP prep in northern ADJ amod northern Alabama PROPN pobj alabama , PUNCT punct , looking VERB advcl look down PART prt down SPACE into ADP prep into the DET det the swift ADJ amod swift water NOUN pobj water twenty NUM nummod twenty feet NOUN npadvmod foot below ADV advmod below . PUNCT punct . SPACE . . A DET det a man NOUN nsubj man stood VERB ROOT stand upon ADP prep upon a DET det a railroad NOUN compound railroad bridge NOUN pobj bridge in ADP prep in northern ADJ amod northern Alabama PROPN pobj alabama , PUNCT punct , looking VERB advcl look down PART prt down SPACE into ADP prep into the DET det the swift ADJ amod swift water NOUN pobj water twenty NUM nummod twenty feet NOUN npadvmod foot below ADV advmod below . PUNCT punct . SPACE . **Solution** . for token in sentences[0]: #print(token.text,token.pos_,token.tag_,token.lemma_) print(f&#39;{token.text:&gt;{10}}{token.pos_:&gt;{10}}{token.dep_:&gt;{10}}{token.lemma_:&gt;{10}}&#39;) . AN DET det an OCCURRENCE NOUN nsubjoccurrence AT ADP prep at OWL PROPN compound OWL CREEK VERB amod CREEK BRIDGE PROPN compound BRIDGE SPACE dep by ADP prep by Ambrose PROPN compound Ambrose Bierce PROPN pobj Bierce SPACE dep I PRON nsubj I SPACE dep A DET det a man NOUN nsubj man stood VERB ROOT stand upon SCONJ prep upon a DET det a railroad NOUN compound railroad bridge NOUN pobj bridge in ADP prep in northern ADJ amod northern Alabama PROPN pobj Alabama , PUNCT punct , looking VERB advcl look down ADV advmod down SPACE dep into ADP prep into the DET det the swift ADJ amod swift water NOUN pobj water twenty NUM nummod twenty feet NOUN npadvmod foot below ADV advmod below . PUNCT punct . . 6. Write a matcher called &#39;Swimming&#39; that finds both occurrences of the phrase &quot;swimming vigorously&quot; in the text HINT: You should include an &#39;IS_SPACE&#39;: True pattern between the two words! . from spacy.matcher import Matcher matcher = Matcher(nlp.vocab) . . . [(12881893835109366681, 1274, 1277), (12881893835109366681, 3607, 3610)] . Solution . pattern = [{&#39;LOWER&#39;: &#39;swimming&#39;},{&#39;IS_SPACE&#39;:True,&#39;OP&#39;:&#39;*&#39;},{&#39;LOWER&#39;:&#39;vigorously&#39;}] matcher.add(&#39;Swimming&#39;,None, pattern) . TypeError Traceback (most recent call last) C: Users VICKY~1.CRA AppData Local Temp/ipykernel_8136/1760381471.py in &lt;module&gt; 1 pattern = [{&#39;LOWER&#39;: &#39;swimming&#39;},{&#39;IS_SPACE&#39;:True,&#39;OP&#39;:&#39;*&#39;},{&#39;LOWER&#39;:&#39;vigorously&#39;}] 2 -&gt; 3 matcher.add(&#39;Swimming&#39;,None, pattern) ~ Anaconda3 lib site-packages spacy matcher matcher.pyx in spacy.matcher.matcher.Matcher.add() TypeError: add() takes exactly 2 positional arguments (3 given) . Note Check the update in the code - https://stackoverflow.com/questions/70321680/typeerror-add-takes-exactly-2-positional-arguments-3-given . pattern = [{&#39;LOWER&#39;: &#39;swimming&#39;}, {&#39;IS_SPACE&#39;: True, &#39;OP&#39;:&#39;*&#39;}, {&#39;LOWER&#39;: &#39;vigorously&#39;}] matcher.add(&#39;Swimming&#39;,[pattern]) . found_matches = matcher(doc) print(found_matches) . [(12881893835109366681, 1274, 1277), (12881893835109366681, 3609, 3612)] . 7. Print the text surrounding each found match . Solution . print(doc[1265:1290]) . By diving I could evade the bullets and, swimming vigorously, reach the bank, take to the woods and get away home . print(doc[3600:3615]) . all this over his shoulder; he was now swimming vigorously with the current . EXTRA CREDIT:Print the sentence that contains each found match . . By diving I could evade the bullets and, swimming vigorously, reach the bank, take to the woods and get away home. . . The hunted man saw all this over his shoulder; he was now swimming vigorously with the current. . Solution . for sent in sentences: if found_matches[0][1]&lt;sent.end: print(sent) break . By diving I could evade the bullets and, swimming vigorously, reach the bank, take to the woods and get away home. . for sent in sentences: if found_matches[1][1]&lt;sent.end: print(sent) break . The hunted man saw all this over his shoulder; he was now swimming vigorously with the current. .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/stopwords/nlp-chapter-2/2022/07/02/2-7-NLP-Basics-Practice.html",
            "relUrl": "/spacy/stopwords/nlp-chapter-2/2022/07/02/2-7-NLP-Basics-Practice.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2.6 Vocabulary and Matching",
            "content": "Rule-based Matching . spaCy offers a rule-matching tool called Matcher that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher. . ### Perform standard imports import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . ### Import the Matcher library from spacy.matcher import Matcher matcher = Matcher(nlp.vocab) . Here `matcher` is an object that pairs to the current `Vocab` object. We can add and remove specific named matchers to `matcher` as needed. . Creating patterns . In literature, the phrase &#39;solar power&#39; might appear as one word or two, with or without a hyphen. In this section we&#39;ll develop a matcher named &#39;SolarPower&#39; that finds all three: . pattern1 = [{&#39;LOWER&#39;: &#39;solarpower&#39;}] pattern2 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;LOWER&#39;: &#39;power&#39;}] pattern3 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;IS_PUNCT&#39;: True}, {&#39;LOWER&#39;: &#39;power&#39;}] matcher.add(&#39;SolarPower&#39;, None, pattern1, pattern2, pattern3) . Let&#39;s break this down: . pattern1 looks for a single token whose lowercase text reads &#39;solarpower&#39; | pattern2 looks for two adjacent tokens that read &#39;solar&#39; and &#39;power&#39; in that order | pattern3 looks for three adjacent tokens, with a middle token that can be any punctuation.* | . * Remember that single spaces are not tokenized, so they don&#39;t count as punctuation. Once we define our patterns, we pass them into matcher with the name &#39;SolarPower&#39;, and set callbacks to None (more on callbacks later). . Applying the matcher to a Doc object . doc = nlp(u&#39;The Solar Power industry continues to grow as demand for solarpower increases. Solar-power cars are gaining popularity.&#39;) . found_matches = matcher(doc) print(found_matches) . [(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)] . matcher returns a list of tuples. Each tuple contains an ID for the match, with start &amp; end tokens that map to the span doc[start:end] . for match_id, start, end in found_matches: string_id = nlp.vocab.strings[match_id] # get string representation span = doc[start:end] # get the matched span print(match_id, string_id, start, end, span.text) . 8656102463236116519 SolarPower 1 3 Solar Power 8656102463236116519 SolarPower 10 11 solarpower 8656102463236116519 SolarPower 13 16 Solar-power . The match_id is simply the hash value of the string_ID &#39;SolarPower&#39; . Setting pattern options and quantifiers . You can make token rules optional by passing an &#39;OP&#39;:&#39;*&#39; argument. This lets us streamline our patterns list: . pattern1 = [{&#39;LOWER&#39;: &#39;solarpower&#39;}] pattern2 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;IS_PUNCT&#39;: True, &#39;OP&#39;:&#39;*&#39;}, {&#39;LOWER&#39;: &#39;power&#39;}] # Remove the old patterns to avoid duplication: matcher.remove(&#39;SolarPower&#39;) # Add the new set of patterns to the &#39;SolarPower&#39; matcher: matcher.add(&#39;SolarPower&#39;, None, pattern1, pattern2) . found_matches = matcher(doc) print(found_matches) . [(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)] . This found both two-word patterns, with and without the hyphen! . The following quantifiers can be passed to the &#39;OP&#39; key: . OPDescription . ! | Negate the pattern, by requiring it to match exactly 0 times | . ? | Make the pattern optional, by allowing it to match 0 or 1 times | . + | Require the pattern to match 1 or more times | . * | Allow the pattern to match zero or more times | . Be careful with lemmas! . If we wanted to match on both &#39;solar power&#39; and &#39;solar powered&#39;, it might be tempting to look for the lemma of &#39;powered&#39; and expect it to be &#39;power&#39;. This is not always the case! The lemma of the adjective &#39;powered&#39; is still &#39;powered&#39;: . pattern1 = [{&#39;LOWER&#39;: &#39;solarpower&#39;}] pattern2 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;IS_PUNCT&#39;: True, &#39;OP&#39;:&#39;*&#39;}, {&#39;LEMMA&#39;: &#39;power&#39;}] # CHANGE THIS PATTERN # Remove the old patterns to avoid duplication: matcher.remove(&#39;SolarPower&#39;) # Add the new set of patterns to the &#39;SolarPower&#39; matcher: matcher.add(&#39;SolarPower&#39;, None, pattern1, pattern2) . doc2 = nlp(u&#39;Solar-powered energy runs solar-powered cars.&#39;) . found_matches = matcher(doc2) print(found_matches) . [(8656102463236116519, 0, 3)] . The matcher found the first occurrence because the lemmatizer treated &#39;Solar-powered&#39; as a verb, but not the second as it considered it an adjective.For this case it may be better to set explicit token patterns. . pattern1 = [{&#39;LOWER&#39;: &#39;solarpower&#39;}] pattern2 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;IS_PUNCT&#39;: True, &#39;OP&#39;:&#39;*&#39;}, {&#39;LOWER&#39;: &#39;power&#39;}] pattern3 = [{&#39;LOWER&#39;: &#39;solarpowered&#39;}] pattern4 = [{&#39;LOWER&#39;: &#39;solar&#39;}, {&#39;IS_PUNCT&#39;: True, &#39;OP&#39;:&#39;*&#39;}, {&#39;LOWER&#39;: &#39;powered&#39;}] # Remove the old patterns to avoid duplication: matcher.remove(&#39;SolarPower&#39;) # Add the new set of patterns to the &#39;SolarPower&#39; matcher: matcher.add(&#39;SolarPower&#39;, None, pattern1, pattern2, pattern3, pattern4) . found_matches = matcher(doc2) print(found_matches) . [(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)] . Other token attributes . Besides lemmas, there are a variety of token attributes we can use to determine matching rules: . AttributeDescription . `ORTH` | The exact verbatim text of a token | . `LOWER` | The lowercase form of the token text | . `LENGTH` | The length of the token text | . `IS_ALPHA`, `IS_ASCII`, `IS_DIGIT` | Token text consists of alphanumeric characters, ASCII characters, digits | . `IS_LOWER`, `IS_UPPER`, `IS_TITLE` | Token text is in lowercase, uppercase, titlecase | . `IS_PUNCT`, `IS_SPACE`, `IS_STOP` | Token is punctuation, whitespace, stop word | . `LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL` | Token text resembles a number, URL, email | . `POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE` | The token&#39;s simple and extended part-of-speech tag, dependency label, lemma, shape | . `ENT_TYPE` | The token&#39;s entity label | . Token wildcard . You can pass an empty dictionary {} as a wildcard to represent any token. For example, you might want to retrieve hashtags without knowing what might follow the # character: . [{&#39;ORTH&#39;:&#39;#&#39;}, {}] . PhraseMatcher . In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into matcher instead. . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . from spacy.matcher import PhraseMatcher matcher = PhraseMatcher(nlp.vocab) . For this exercise we&#39;re going to import a Wikipedia article on Reaganomics Source: https://en.wikipedia.org/wiki/Reaganomics . with open(&#39;../TextFiles/reaganomics.txt&#39;, encoding=&#39;utf8&#39;) as f: doc3 = nlp(f.read()) . phrase_list = [&#39;voodoo economics&#39;, &#39;supply-side economics&#39;, &#39;trickle-down economics&#39;, &#39;free-market economics&#39;] # Next, convert each phrase to a Doc object: phrase_patterns = [nlp(text) for text in phrase_list] # Pass each Doc object into matcher (note the use of the asterisk!): matcher.add(&#39;VoodooEconomics&#39;, None, *phrase_patterns) # Build a list of matches: matches = matcher(doc3) . matches . [(3473369816841043438, 41, 45), (3473369816841043438, 49, 53), (3473369816841043438, 54, 56), (3473369816841043438, 61, 65), (3473369816841043438, 673, 677), (3473369816841043438, 2985, 2989)] . The first four matches are where these terms are used in the definition of Reaganomics: . doc3[:70] . REAGANOMICS https://en.wikipedia.org/wiki/Reaganomics Reaganomics (a portmanteau of [Ronald] Reagan and economics attributed to Paul Harvey)[1] refers to the economic policies promoted by U.S. President Ronald Reagan during the 1980s. These policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates. . Viewing Matches . There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc that is wider than the match: . doc3[665:685] # Note that the fifth match starts at doc3[673] . same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian . doc3[2975:2995] # The sixth match starts at doc3[2985] . against institutions.[66] His policies became widely known as &#34;trickle-down economics&#34;, due to the significant . Another way is to first apply the sentencizer to the Doc, then iterate through the sentences to the match point: . sents = [sent for sent in doc3.sents] ### In the next section we&#39;ll see that sentences contain start and end token values: print(sents[0].start, sents[0].end) . 0 35 . for sent in sents: if matches[4][1] &lt; sent.end: # this is the fifth match, that starts at doc3[673] print(sent) break . At the same time he attracted a following from the supply-side economics movement, which formed in opposition to Keynesian demand-stimulus economics. . For additional information visit https://spacy.io/usage/linguistic-features#section-rule-based-matching .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/stopwords/nlp-chapter-2/word-matcher/phrase-matcher/2022/07/01/2-6-Vocabulary-and-Matching.html",
            "relUrl": "/spacy/stopwords/nlp-chapter-2/word-matcher/phrase-matcher/2022/07/01/2-6-Vocabulary-and-Matching.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2.5 Stop Words",
            "content": "Words like &quot;a&quot; and &quot;the&quot; appear so frequently that they don&#39;t require tagging as thoroughly as nouns, verbs and modifiers. We call these stop words, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 305 English stop words . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . Print the set of spaCy&#39;s default stop words . print(nlp.Defaults.stop_words) . {&#39;whereafter&#39;, &#39;her&#39;, &#39;although&#39;, &#39;across&#39;, &#39;make&#39;, &#39;be&#39;, &#39;wherein&#39;, &#39;hereby&#39;, &#39;latterly&#39;, &#39;last&#39;, &#39;ours&#39;, &#39;‘ll&#39;, &#39;twelve&#39;, &#39;behind&#39;, &#39;they&#39;, &#39;did&#39;, &#39;herself&#39;, &#39;anyhow&#39;, &#39;namely&#39;, &#39;him&#39;, &#39;always&#39;, &#39;however&#39;, &#39;or&#39;, &#39;ever&#39;, &#39;ten&#39;, &#39;again&#39;, &#39;almost&#39;, &#39;has&#39;, &#39;formerly&#39;, &#39;hereafter&#39;, &#39;do&#39;, &#39;anyway&#39;, &#39;cannot&#39;, &#39;due&#39;, &#39;via&#39;, &#39;more&#39;, &#39;whatever&#39;, &#39;my&#39;, &#39;by&#39;, &#39;get&#39;, &#34;&#39;d&#34;, &#39;some&#39;, &#39;were&#39;, &#39;name&#39;, &#39;seeming&#39;, &#39;mostly&#39;, &#39;same&#39;, &#39;often&#39;, &#39;wherever&#39;, &#39;six&#39;, &#39;yours&#39;, &#39;keep&#39;, &#39;everywhere&#39;, &#39;here&#39;, &#39;thus&#39;, &#39;used&#39;, &#39;part&#39;, &#39;anything&#39;, &#39;over&#39;, &#39;for&#39;, &#39;upon&#39;, &#39;until&#39;, &#39;seem&#39;, &#39;onto&#39;, &#39;can&#39;, &#39;above&#39;, &#39;this&#39;, &#39;not&#39;, &#39;down&#39;, &#39;yet&#39;, &#39;something&#39;, &#39;further&#39;, &#39;amongst&#39;, &#39;whether&#39;, &#39;she&#39;, &#39;someone&#39;, &#39;if&#39;, &#39;beforehand&#39;, &#39;should&#39;, &#39;all&#39;, &#39;whence&#39;, &#39;becomes&#39;, &#39;one&#39;, &#39;‘m&#39;, &#39;while&#39;, &#39;another&#39;, &#34;&#39;m&#34;, &#39;‘ve&#39;, &#39;before&#39;, &#39;‘s&#39;, &#39;anywhere&#39;, &#39;go&#39;, &#39;first&#39;, &#39;sixty&#39;, &#39;whereupon&#39;, &#39;does&#39;, &#39;your&#39;, &#39;nothing&#39;, &#39;least&#39;, &#39;but&#39;, &#39;various&#39;, &#39;anyone&#39;, &#39;been&#39;, &#39;might&#39;, &#39;though&#39;, &#39;why&#39;, &#39;say&#39;, &#39;see&#39;, &#39;both&#39;, &#39;beyond&#39;, &#39;somewhere&#39;, &#39;its&#39;, &#39;five&#39;, &#39;afterwards&#39;, &#39;seems&#39;, &#39;these&#39;, &#39;even&#39;, &#39;ourselves&#39;, &#39;still&#39;, &#39;nowhere&#39;, &#39;next&#39;, &#39;top&#39;, &#39;those&#39;, &#39;became&#39;, &#39;quite&#39;, &#39;yourself&#39;, &#39;well&#39;, &#39;fifteen&#39;, &#39;he&#39;, &#39;amount&#39;, &#39;moreover&#39;, &#39;other&#39;, &#34;&#39;re&#34;, &#39;much&#39;, &#39;nevertheless&#39;, &#39;themselves&#39;, &#39;must&#39;, &#39;mine&#39;, &#39;an&#39;, &#39;whose&#39;, &#39;thereupon&#39;, &#39;made&#39;, &#39;therein&#39;, &#39;own&#39;, &#39;whenever&#39;, &#39;except&#39;, &#39;to&#39;, &#39;’ll&#39;, &#39;at&#39;, &#39;under&#39;, &#39;elsewhere&#39;, &#39;alone&#39;, &#39;eight&#39;, &#39;now&#39;, &#39;without&#39;, &#39;otherwise&#39;, &#39;empty&#39;, &#39;four&#39;, &#39;us&#39;, &#34;&#39;ve&#34;, &#39;then&#39;, &#39;eleven&#39;, &#39;too&#39;, &#39;n‘t&#39;, &#34;&#39;ll&#34;, &#39;thence&#39;, &#39;full&#39;, &#39;as&#39;, &#39;their&#39;, &#39;in&#39;, &#39;out&#39;, &#39;along&#39;, &#39;’re&#39;, &#39;’s&#39;, &#39;whither&#39;, &#39;whoever&#39;, &#39;put&#39;, &#39;since&#39;, &#39;itself&#39;, &#39;never&#39;, &#39;around&#39;, &#39;hence&#39;, &#39;during&#39;, &#39;everything&#39;, &#39;about&#39;, &#39;using&#39;, &#39;himself&#39;, &#39;yourselves&#39;, &#39;doing&#39;, &#39;against&#39;, &#39;also&#39;, &#39;being&#39;, &#39;which&#39;, &#39;we&#39;, &#39;with&#39;, &#39;side&#39;, &#39;besides&#39;, &#39;re&#39;, &#39;very&#39;, &#39;forty&#39;, &#39;through&#39;, &#39;sometimes&#39;, &#39;unless&#39;, &#39;together&#39;, &#39;serious&#39;, &#39;sometime&#39;, &#39;move&#39;, &#39;up&#39;, &#39;throughout&#39;, &#39;would&#39;, &#39;latter&#39;, &#39;two&#39;, &#39;meanwhile&#39;, &#39;therefore&#39;, &#39;such&#39;, &#39;where&#39;, &#39;whereby&#39;, &#39;who&#39;, &#39;many&#39;, &#39;how&#39;, &#39;thereby&#39;, &#39;twenty&#39;, &#39;from&#39;, &#39;back&#39;, &#39;into&#39;, &#39;the&#39;, &#39;else&#39;, &#39;herein&#39;, &#39;becoming&#39;, &#39;myself&#39;, &#39;nor&#39;, &#39;am&#39;, &#39;what&#39;, &#39;done&#39;, &#39;just&#39;, &#39;is&#39;, &#39;seemed&#39;, &#39;every&#39;, &#39;’m&#39;, &#39;already&#39;, &#39;that&#39;, &#34;n&#39;t&#34;, &#39;hundred&#39;, &#39;within&#39;, &#39;on&#39;, &#39;call&#39;, &#39;beside&#39;, &#39;whole&#39;, &#39;among&#39;, &#39;please&#39;, &#39;others&#39;, &#39;ca&#39;, &#39;’ve&#39;, &#39;me&#39;, &#39;n’t&#39;, &#34;&#39;s&#34;, &#39;none&#39;, &#39;several&#39;, &#39;few&#39;, &#39;indeed&#39;, &#39;so&#39;, &#39;regarding&#39;, &#39;most&#39;, &#39;whom&#39;, &#39;whereas&#39;, &#39;’d&#39;, &#39;nine&#39;, &#39;may&#39;, &#39;between&#39;, &#39;once&#39;, &#39;i&#39;, &#39;will&#39;, &#39;no&#39;, &#39;only&#39;, &#39;either&#39;, &#39;towards&#39;, &#39;was&#39;, &#39;our&#39;, &#39;a&#39;, &#39;three&#39;, &#39;had&#39;, &#39;thereafter&#39;, &#39;there&#39;, &#39;are&#39;, &#39;when&#39;, &#39;front&#39;, &#39;than&#39;, &#39;less&#39;, &#39;‘d&#39;, &#39;nobody&#39;, &#39;per&#39;, &#39;take&#39;, &#39;neither&#39;, &#39;could&#39;, &#39;it&#39;, &#39;enough&#39;, &#39;after&#39;, &#39;off&#39;, &#39;hers&#39;, &#39;former&#39;, &#39;below&#39;, &#39;bottom&#39;, &#39;become&#39;, &#39;hereupon&#39;, &#39;noone&#39;, &#39;show&#39;, &#39;give&#39;, &#39;third&#39;, &#39;everyone&#39;, &#39;somehow&#39;, &#39;them&#39;, &#39;each&#39;, &#39;any&#39;, &#39;because&#39;, &#39;have&#39;, &#39;rather&#39;, &#39;really&#39;, &#39;and&#39;, &#39;you&#39;, &#39;of&#39;, &#39;‘re&#39;, &#39;toward&#39;, &#39;his&#39;, &#39;perhaps&#39;, &#39;thru&#39;, &#39;fifty&#39;} . len(nlp.Defaults.stop_words) . 326 . Check if a word is stop word or not . nlp.vocab[&#39;myself&#39;].is_stop . True . nlp.vocab[&#39;elephant&#39;].is_stop . False . . Step 1 - Add the word to the set of stop words. Use lowercase! . nlp.Defaults.stop_words.add(&#39;btw&#39;) . Step 2 - Set the stop word tag on the lexeme . nlp.vocab[&#39;btw&#39;].is_stop = True . len(nlp.Defaults.stop_words) . 327 . nlp.vocab[&#39;btw&#39;].is_stop . True . When adding stop words, always use lowercase. Lexemes are converted to lowercase before being added to **vocab**. . Removing a stop word from the default list. . Step 1 - Remove the word from the set of stop words . nlp.Defaults.stop_words.remove(&#39;beyond&#39;) . Step 2 - Remove the stop_word tag from the lexeme . nlp.vocab[&#39;beyond&#39;].is_stop = False . len(nlp.Defaults.stop_words) . 326 . nlp.vocab[&#39;beyond&#39;].is_stop . False .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/stopwords/nlp-chapter-2/2022/07/01/2-5-Stop-words.html",
            "relUrl": "/spacy/stopwords/nlp-chapter-2/2022/07/01/2-5-Stop-words.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2.4 Lemmatization",
            "content": "In contrast to stemming, lemmatization looks beyond word reduction, and considers a language&#39;s full vocabulary to apply a morphological analysis to words. The lemma of &#39;was&#39; is &#39;be&#39; and the lemma of &#39;mice&#39; is &#39;mouse&#39;. Further, the lemma of &#39;meeting&#39; might be &#39;meet&#39; or &#39;meeting&#39; depending on its use in a sentence. . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . doc1 = nlp(u&quot;I am a runner running in a race because I love to run since I ran today&quot;) for token in doc1: print(f&#39;{token.text:&lt;{10}}{token.pos_:&lt;{10}}{token.lemma:&lt;{25}}{token.lemma_:&lt;{10}}&#39;) . I PRON 4690420944186131903 I am AUX 10382539506755952630 be a DET 11901859001352538922 a runner NOUN 12640964157389618806 runner running VERB 12767647472892411841 run in ADP 3002984154512732771 in a DET 11901859001352538922 a race NOUN 8048469955494714898 race because SCONJ 16950148841647037698 because I PRON 4690420944186131903 I love VERB 3702023516439754181 love to PART 3791531372978436496 to run VERB 12767647472892411841 run since SCONJ 10066841407251338481 since I PRON 4690420944186131903 I ran VERB 12767647472892411841 run today NOUN 11042482332948150395 today . In this case we see that running, run and ran have the same lemma (12767647472892411841) . . def show_lemmas(text): for token in text: print(f&#39;{token.text:{12}} {token.pos_:{6}} {token.lemma:&lt;{22}} {token.lemma_}&#39;) . doc2 = nlp(u&quot;I saw eighteen mice today!&quot;) show_lemmas(doc2) . I PRON 4690420944186131903 I saw VERB 11925638236994514241 see eighteen NUM 9609336664675087640 eighteen mice NOUN 1384165645700560590 mouse today NOUN 11042482332948150395 today ! PUNCT 17494803046312582752 ! . doc3 = nlp(u&quot;I am meeting him tomorrow at the meeting.&quot;) show_lemmas(doc3) . I PRON 4690420944186131903 I am AUX 10382539506755952630 be meeting VERB 6880656908171229526 meet him PRON 1655312771067108281 he tomorrow NOUN 3573583789758258062 tomorrow at ADP 11667289587015813222 at the DET 7425985699627899538 the meeting NOUN 14798207169164081740 meeting . PUNCT 12646065887601541794 . . Here we see how meeting is correctly tagged as a noun and a verb . doc4 = nlp(u&quot;That&#39;s an enormous automobile&quot;) show_lemmas(doc4) . That PRON 4380130941430378203 that &#39;s AUX 10382539506755952630 be an DET 15099054000809333061 an enormous ADJ 17917224542039855524 enormous automobile NOUN 7211811266693931283 automobile . Note that lemmatization does *not* reduce words to their most basic synonym - that is, `enormous` doesn&#39;t become `big` and `automobile` doesn&#39;t become `car`. .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/nlp-chapter-2/lemma/2022/07/01/2-4-Lemmatization.html",
            "relUrl": "/spacy/nlp-chapter-2/lemma/2022/07/01/2-4-Lemmatization.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2.3 Stemming",
            "content": "Stemming is the process of removing a part of a word, or reducing a word to its stem or root. . Porter Stemmer . One of the most common - and effective - stemming tools is Porter&#39;s Algorithm developed by Martin Porter in 1980. The algorithm employs five phases of word reduction, each with its own set of mapping rules. In the first phase, simple suffix mapping rules are defined, such as: . . From a given set of stemming rules only one rule is applied, based on the longest suffix S1. Thus, caresses reduces to caress but not cares. . More sophisticated phases consider the length/complexity of the word before applying a rule. For example: . . Porter stemmer using NLKT . import nltk from nltk.stem.porter import * . p_stemmer = PorterStemmer() . words = [&#39;run&#39;,&#39;runner&#39;,&#39;running&#39;,&#39;ran&#39;,&#39;runs&#39;,&#39;easily&#39;,&#39;fairly&#39;] . for word in words: print(word+&#39; &gt; &#39;+p_stemmer.stem(word)) . run &gt; run runner &gt; runner running &gt; run ran &gt; ran runs &gt; run easily &gt; easili fairly &gt; fairli . Snowball Stemmer . This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more acurately called the &quot;English Stemmer&quot; or &quot;Porter2 Stemmer&quot;. It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since nltk uses the name SnowballStemmer, we&#39;ll use it here. . from nltk.stem.snowball import SnowballStemmer # The Snowball Stemmer requires that you pass a language parameter s_stemmer = SnowballStemmer(language=&#39;english&#39;) . words = [&#39;run&#39;,&#39;runner&#39;,&#39;running&#39;,&#39;ran&#39;,&#39;runs&#39;,&#39;easily&#39;,&#39;fairly&#39;] # words = [&#39;generous&#39;,&#39;generation&#39;,&#39;generously&#39;,&#39;generate&#39;] . for word in words: print(word+&#39; --&gt; &#39;+s_stemmer.stem(word)) . run --&gt; run runner --&gt; runner running --&gt; run ran --&gt; ran runs --&gt; run easily --&gt; easili fairly --&gt; fair .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/nltk/stemming/nlp-chapter-2/2022/07/01/2-3-Stemming.html",
            "relUrl": "/nltk/stemming/nlp-chapter-2/2022/07/01/2-3-Stemming.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2.2 Tokenization",
            "content": "The first step of creating Doc object is breakdown the raw text into smaller pieces or tokens . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . mystring = &#39;&quot;We &#39;re moving to L.A.!&quot;&#39; print(mystring) . &#34;We&#39;re moving to L.A.!&#34; . doc = nlp(mystring) for token in doc: print(token.text, end = &quot; | &quot;) . &#34; | We | &#39;re | moving | to | L.A. | ! | &#34; | . Spacy follows the following sequence to break the text. . . Prefix: Character(s) at the beginning &#9656; $ ( “ ¿ | Suffix: Character(s) at the end &#9656; km ) , . ! ” | Infix: Character(s) in between &#9656; - -- / ... | Exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S. | . Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another. . Prefixes, Suffixes and Infixes . spaCy will isolate punctuation that does not form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token. . doc2 = nlp(u&quot;We&#39;re here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!&quot;) for t in doc2: print(t) . We &#39;re here to help ! Send snail - mail , email support@oursite.com or visit us at http://www.oursite.com ! . Note - We see that dash, !, commas are assigned as seperate tokens. But email address and website and kept together. . doc3 = nlp(u&#39;A 5km NYC cab ride costs $10.30&#39;) for t in doc3: print(t) . A 5 km NYC cab ride costs $ 10.30 . Hence the dollar sign and amount are given seperate tokens. . Exceptions . Punctuation that exists as part of a known abbreviation will be kept as part of the token. . doc4 = nlp(u&quot;Let&#39;s visit St. Louis in the U.S. next year.&quot;) for t in doc4: print(t) . Let &#39;s visit St. Louis in the U.S. next year . . In this case we see abbrevation such St. and U.S are preseved in seperate tokens. . Counting tokens . len(doc) . 8 . Counting Vocab Entrmies . len(doc.vocab) . 802 . Retriving tokens by index position and slice . doc5 = nlp(u&#39;It is better to give than to receive.&#39;) ### Retrieve the third token: doc5[2] . better . doc5[2:5] . better to give . . doc6 = nlp(u&#39;My dinner was horrible.&#39;) doc7 = nlp(u&#39;Your dinner was delicious.&#39;) . doc6[3] = doc7[3] . TypeError Traceback (most recent call last) C: Users VICKY~1.CRA AppData Local Temp/ipykernel_11844/3591796456.py in &lt;module&gt; -&gt; 1 doc6[3] = doc7[3] TypeError: &#39;spacy.tokens.doc.Doc&#39; object does not support item assignment . In this case we are trying to replace horrible in doc6 with delicious from doc7. But it cannot be done. . Named Entities . The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the ents property of a Doc object. . doc8 = nlp(u&#39;Apple to build a Hong Kong factory for $6 million&#39;) for token in doc8: print(token.text, end=&#39; | &#39;) print(&#39; n-&#39;) for ent in doc8.ents: print(ent.text+&#39; - &#39;+ent.label_+&#39; - &#39;+str(spacy.explain(ent.label_))) . Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | - Hong Kong - GPE - Countries, cities, states $6 million - MONEY - Monetary values, including unit . . Doc.noun_chunks are another object property. Noun chunks are &quot;base noun phrases&quot; – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in Sheb Wooley&#39;s 1958 song, a &quot;one-eyed, one-horned, flying, purple people-eater&quot; would be one long noun chunk. . doc9 = nlp(u&quot;Autonomous cars shift insurance liability toward manufacturers.&quot;) for chunk in doc9.noun_chunks: print(chunk.text) . Autonomous cars insurance liability manufacturers . doc10 = nlp(u&quot;Red cars do not carry higher insurance rates.&quot;) for chunk in doc10.noun_chunks: print(chunk.text) . Red cars higher insurance rates . doc11 = nlp(u&quot;He was a one-eyed, one-horned, flying, purple people-eater.&quot;) for chunk in doc11.noun_chunks: print(chunk.text) . He purple people-eater . Built in Visualizers . spaCy includes a built-in visualization tool called displaCy . from spacy import displacy doc = nlp(u&#39;Apple is going to build a U.K. factory for $6 million.&#39;) displacy.render(doc, style=&#39;dep&#39;, jupyter=True, options={&#39;distance&#39;: 120}) . Apple PROPN is AUX going VERB to PART build VERB a DET U.K. PROPN factory NOUN for ADP $ SYM 6 NUM million. NUM nsubj aux aux xcomp det compound dobj prep quantmod compound pobj The optional &#39;distance&#39; argument sets the distance between tokens. If the distance is made too small, text that appears beneath short arrows may become too compressed to read. . Visualizing the entity recognizer . doc = nlp(u&#39;Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.&#39;) displacy.render(doc, style=&#39;ent&#39;, jupyter=True) . Over the last quarter DATE Apple ORG sold nearly 20 thousand CARDINAL iPods PRODUCT for a profit of $6 million MONEY .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/nlp-chapter-2/2022/07/01/2-2-Tokenization.html",
            "relUrl": "/spacy/nlp-chapter-2/2022/07/01/2-2-Tokenization.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2.1 Spacy Basics",
            "content": "Installation and setup . pip install -U spacy . Downloading spacy vocab library . !python -m spacy download en . Loading Spacy . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . Creating a doc object and printing the different components of the token . ### Create a doc object doc = nlp(u&#39;Tesla is looking at buying U.S. startup for $6 million&#39;) ### Printing each token seperately for token in doc: print(f&#39;{token.text:&gt;{10}} {token.pos_:&gt;{10}} {token.dep_:&gt;{10}}&#39;) . Tesla NOUN nsubj is AUX aux looking VERB ROOT at ADP prep buying VERB pcomp U.S. PROPN compound startup NOUN dobj for ADP prep $ SYM quantmod 6 NUM compound million NUM pobj . Understanding the spacy pipeline . When we run nlp, our text enters a processing pipeline that first breaks down the text and then performs a series of operations to tag, parse and describe the data. Image source: https://spacy.io/usage/spacy-101#pipelines . . NAME COMPONENT CREATES DESCRIPTION . tokenizer | Tokenizer | Doc | Segment text into tokens. | . tagger | Tagger | Token.tag | Assign part-of-speech tags. | . parser | DependencyParser | Token.head,&nbsp;Token.dep,&nbsp;Doc.sents,&nbsp;Doc.noun_chunks | Assign dependency labels. | . ner | EntityRecognizer | Doc.ents,&nbsp;Token.ent_iob,&nbsp;Token.ent_type | Detect and label named entities. | . lemmatizer | Lemmatizer | Token.lemma | Assign base forms. | . textcat | TextCategorizer | Doc.cats | Assign document labels. | . custom | custom components | Doc._.xxx,&nbsp;Token._.xxx,&nbsp;Span._.xxx | Assign custom attributes, methods or properties. | . nlp.pipeline . [(&#39;tok2vec&#39;, &lt;spacy.pipeline.tok2vec.Tok2Vec at 0x23264bfa220&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x23264bfae80&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x23264a554a0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x23264cb5780&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x23264cc8780&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x23264a55350&gt;)] . nlp.pipe_names . [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;attribute_ruler&#39;, &#39;lemmatizer&#39;, &#39;ner&#39;] . Tokenization . Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words. . Source: https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4s . doc2 = nlp(u&quot;Apple isn&#39;t looking into buying startups.&quot;) for token in doc2: print(f&#39;{token.text:&gt;{10}} {token.pos_:&gt;{10}} {token.dep_:&gt;{10}}&#39;) . Apple PROPN nsubj is AUX aux n&#39;t PART neg SPACE dep looking VERB ROOT into ADP prep buying VERB pcomp startups NOUN dobj . PUNCT punct . Things to take note . Spacy is able to recognize the root verb and the negation, hence it has split is&#39;nt into two tokens | Spaces and peroid are assigned as tokens | . type(doc2) . spacy.tokens.doc.Doc . doc2 is spacy object and contains information of each token in the text. . Part of Speech Tagging(POS) . In the above example we see the output has clearly label Apple as pronoun, looking as a verb etc.These are parts of speech. . For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging . Dependencies . We also looked at the syntactic dependencies assigned to each token. Tesla is identified as an nsubj or the nominal subject of the sentence. . For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing A good explanation of typed dependencies can be found here . Full name of a tag used in spacy . spacy.explain(&#39;PROPN&#39;) . &#39;proper noun&#39; . spacy.explain(&#39;nsubj&#39;) . &#39;nominal subject&#39; . Additional Token Attributes . Tag Description doc2[0].tag . .text | The original word text | Tesla | . .lemma_ | The base form of the word | tesla | . .pos_ | The simple part-of-speech tag | PROPN/proper noun | . .tag_ | The detailed part-of-speech tag | NNP/noun, proper singular | . .shape_ | The word shape – capitalization, punctuation, digits | Xxxxx | . .is_alpha | Is the token an alpha character? | True | . .is_stop | Is the token part of a stop list, i.e. the most common words of the language? | False | . print(doc2[4].text) print(doc2[4].lemma_) . looking look . print(doc2[0].text + &#39;: &#39; + doc2[0].shape_) print(doc[5].text + &#39;: &#39; + doc[5].shape_) . Apple: Xxxxx U.S.: X.X. . . Large Doc objects can be hard to work with at times. A span is a slice of Doc object in the form Doc[start:stop]. . doc3 = nlp(u&#39;Although commmonly attributed to John Lennon from his song &quot;Beautiful Boy&quot;, the phrase &quot;Life is what happens to us while we are making other plans&quot; was written by cartoonist Allen Saunders and published in Reader &#39;s Digest in 1957, when Lennon was 17.&#39;) . life_qoute = doc3[16:30] print(life_qoute) . &#34;Life is what happens to us while we are making other plans&#34; . type(life_qoute) . spacy.tokens.span.Span . . Certain tokens inside a Doc object may also receive a &quot;start of sentence&quot; tag. While this doesn&#39;t immediately build a list of sentences, these tags enable the generation of sentence segments through Doc.sents. . doc4 = nlp(u&#39;This is the first sentence. This is another sentence. This is the last sentence.&#39;) . for sent in doc4.sents: print(sent) . This is the first sentence. This is another sentence. This is the last sentence. . doc4[6].is_sent_start . True .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/nlp-chapter-2/2022/07/01/2-1-Spacy-Basics.html",
            "relUrl": "/spacy/nlp-chapter-2/2022/07/01/2-1-Spacy-Basics.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "1.5 Text Basics - Practice exercises",
            "content": "f-Strings . 1. Print an f-string that displays NLP stands for Natural Language Processing using the variables provided. . abbr = &#39;NLP&#39; full_text = &#39;Natural Language Processing&#39; ### Enter your code here: . NLP stands for Natural Language Processing . Solution . abbr = &#39;NLP&#39; full_text = &#39;Natural Language Processing&#39; print(f&#39;{abbr} stands for {full_text}&#39;) . NLP stands for Natural Language Processing . Files . 2. Create a file in the current working directory called contacts.txt by running the cell below: . %%writefile contacts.txt First_Name Last_Name, Title, Extension, Email . Overwriting contacts.txt . Solution . %%writefile contacts.txt First_Name Last_Name, Title, Extension, Email . Overwriting contacts.txt . 3. Open the file and use .read() to save the contents of the file to a string called fields. Make sure the file is closed at the end. . ### Write your code here: ### Run fields to see the contents of contacts.txt: fields . &#39;First_Name Last_Name, Title, Extension, Email&#39; . Solution . with open(&quot;contacts.txt&quot;) as text: fields = text.read() fields . &#39;First_Name Last_Name, Title, Extension, Email n&#39; . Working with PDF Files . 4. Use PyPDF2 to open the file Business_Proposal.pdf. Extract the text of page 2. . # Open the file as a binary object # Use PyPDF2 to read the text of the file # Get the text from page 2 (CHALLENGE: Do this in one step!) page_two_text = # Close the file # Print the contents of page_two_text print(page_two_text) . AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . Solution . import PyPDF2 # Open the file as a binary object pdf1 = open(&quot;data_files/Business_Proposal.pdf&quot;, &#39;rb&#39;) # Use PyPDF2 to read the text of the file pdf_reader = PyPDF2.PdfFileReader(pdf1) # Get the text from page 2 (CHALLENGE: Do this in one step!) page_two_text = pdf_reader.getPage(1).extractText() # Close the file pdf1.close() # Print the contents of page_two_text print(page_two_text) . AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . import re re.findall(r&#39;[^(AUTHORS:)]&#39;, page_two_text) . [&#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;m&#39;, &#39;y&#39;, &#39; &#39;, &#39;B&#39;, &#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;r&#39;, &#39;,&#39;, &#39; &#39;, &#39;F&#39;, &#39;i&#39;, &#39;n&#39;, &#39;a&#39;, &#39;n&#39;, &#39;c&#39;, &#39;e&#39;, &#39; &#39;, &#39;C&#39;, &#39;h&#39;, &#39;a&#39;, &#39;i&#39;, &#39;r&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;,&#39;, &#39; &#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;r&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;C&#39;, &#39;h&#39;, &#39;r&#39;, &#39;i&#39;, &#39;s&#39;, &#39; &#39;, &#39;D&#39;, &#39;o&#39;, &#39;n&#39;, &#39;a&#39;, &#39;l&#39;, &#39;d&#39;, &#39;s&#39;, &#39;o&#39;, &#39;n&#39;, &#39;,&#39;, &#39; &#39;, &#39;c&#39;, &#39;c&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;t&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39; &#39;, &#39;D&#39;, &#39;i&#39;, &#39;r&#39;, &#39;.&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;6&#39;, &#39;2&#39;, &#39;1&#39;, &#39;,&#39;, &#39; &#39;, &#39;c&#39;, &#39;d&#39;, &#39;o&#39;, &#39;n&#39;, &#39;a&#39;, &#39;l&#39;, &#39;d&#39;, &#39;s&#39;, &#39;o&#39;, &#39;n&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;E&#39;, &#39;r&#39;, &#39;i&#39;, &#39;n&#39;, &#39; &#39;, &#39;F&#39;, &#39;r&#39;, &#39;e&#39;, &#39;e&#39;, &#39;m&#39;, &#39;a&#39;, &#39;n&#39;, &#39;,&#39;, &#39; &#39;, &#39;r&#39;, &#39;.&#39;, &#39; &#39;, &#39;V&#39;, &#39;P&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;8&#39;, &#39;7&#39;, &#39;9&#39;, &#39;,&#39;, &#39; &#39;, &#39;e&#39;, &#39;f&#39;, &#39;r&#39;, &#39;e&#39;, &#39;e&#39;, &#39;m&#39;, &#39;a&#39;, &#39;n&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;] . 5. Open the file contacts.txt in append mode. Add the text of page 2 from above to contacts.txt. . CHALLENGE: See if you can remove the word &quot;AUTHORS:&quot; . . First_Name Last_Name, Title, Extension, EmailAUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . **Simple Solution** . myfile = open(&#39;contacts.txt&#39;, &#39;a+&#39;) myfile.seek(0) print(myfile.read()) . First_Name Last_Name, Title, Extension, Email . myfile.write(page_two_text) myfile.seek(0) print(myfile.read()) . First_Name Last_Name, Title, Extension, Email AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . myfile.close() . . First_Name Last_Name, Title, Extension, Email Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . with open(&#39;contacts.txt&#39;,&#39;a+&#39;) as c: c.write(page_two_text[8:]) c.seek(0) print(c.read()) . First_Name Last_Name, Title, Extension, Email AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . Regular Expressions . 6. Using the page_two_text variable created above, extract any email addresses that were contained in the file Business_Proposal.pdf. . import re # Enter your regex pattern here. This may take several tries! pattern = re.findall(pattern, page_two_text) . [&#39;abaker@ourcompany.com&#39;, &#39;cdonaldson@ourcompany.com&#39;, &#39;efreeman@ourcompany.com&#39;] . Solution . import re pattern = r&#39; w+@ w+.com&#39; re.findall(pattern, page_two_text) . [&#39;abaker@ourcompany.com&#39;, &#39;cdonaldson@ourcompany.com&#39;, &#39;efreeman@ourcompany.com&#39;] .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/pypdf2/pdf/nlp-chapter-1/2022/06/30/1-5-Python-Text-Basics-Practice.html",
            "relUrl": "/pypdf2/pdf/nlp-chapter-1/2022/06/30/1-5-Python-Text-Basics-Practice.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "1.4 Regular Expression",
            "content": "Regular exp. library . import re . Finding the first instance in the text . text = &quot;The phone number given in the helpline is 408-999-4567&quot; pattern = &#39;phone&#39; re.search(pattern, text) . &lt;re.Match object; span=(4, 9), match=&#39;phone&#39;&gt; . If the match is found then search return the location of the match. Note: It only gives the first instance in the text. . Span is the starting and ending index of the match. (Index starts from zero) . match=re.search(pattern, text) match . &lt;re.Match object; span=(4, 9), match=&#39;phone&#39;&gt; . .span() give the span of the match, .start() give the start index, .end() gives the end index . match.span() . (4, 9) . match.start() . 4 . match.end() . 9 . Find all instances in the text . text1 = &quot;My phone is a hi-tech phone. The phone is dual band, with the lastest phone-tech processor&quot; . matches = re.findall(&quot;phone&quot;, text1) matches . [&#39;phone&#39;, &#39;phone&#39;, &#39;phone&#39;, &#39;phone&#39;] . len(matches) . 4 . . for match in re.finditer(&#39;phone&#39;, text1): print(match.span()) . (3, 8) (22, 27) (33, 38) (70, 75) . To find the word matched, use .group() method . match.group() . &#39;phone&#39; . Identifiers in Regex . CharacterDescriptionExample Pattern CodeExammple Match . &lt;/p&gt; d | A digit | file_ d d | file_25 | . w | Alphanumeric | w- w w w | A-b_1 | . s | White space | a sb sc | a b c | . D | A non digit | D D D | ABC | . W | Non-alphanumeric | W W W W W | *-+=) | . S | Non-whitespace | S S S S | Yoyo | . &lt;/table&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; text . &#39;The phone number given in the helpline is 408-999-4567&#39; . If we want to find phone number with the pattern xxx-xxx-xxxx, we can use the identifier for it. . re.search(r&#39; d d d- d d d- d d d d&#39;, text).group() . &#39;408-999-4567&#39; . Quantifiers in Regex . In repeating the identifier, we can use quantifiers to do the same thing. . CharacterDescriptionExample Pattern CodeExammple Match . &lt;/p&gt; + | Occurs one or more times | Version w- w+ | Version A-b1_1 | . {3} | Occurs exactly 3 times | D{3} | abc | . {2,4} | Occurs 2 to 4 times | d{2,4} | 123 | . {3,} | Occurs 3 or more | w{3,} | anycharacters | . * | Occurs zero or more times | A *B *C* | AAACC | . ? | Once or none | plurals? | plural | . &lt;/table&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; re.search(r&#39; d{3}- d{3}- d{4}&#39;, text).group() . &#39;408-999-4567&#39; . Groups in Regex search . Using parentheses in regex we can create groups with the matched data . phone_pattern = re.compile(r&#39;( d{3})-( d{3})-( d{4})&#39;) . results = re.search(phone_pattern, text) . results.group() . &#39;408-999-4567&#39; . Each parentheses in the regex pattern is group which can called out. . results.group(1) . &#39;408&#39; . results.group(2) . &#39;999&#39; . results.group(3) . &#39;4567&#39; . Or operator | . re.search(r&quot;man|woman&quot;, &quot;This man is a good person&quot;) . &lt;re.Match object; span=(5, 8), match=&#39;man&#39;&gt; . re.search(r&quot;man|woman&quot;, &quot;This woman is a good person&quot;) . &lt;re.Match object; span=(5, 10), match=&#39;woman&#39;&gt; . Wildcard characters . re.findall(r&quot;.at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39;fat&#39;, &#39;cat&#39;, &#39; at&#39;, &#39;sat&#39;, &#39;rat&#39;, &#39;lat&#39;] . We see that all 3 letter word being matched. One single period matches on wildcard letter before the pattern. . re.findall(r&quot;..at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39; fat&#39;, &#39; cat&#39;, &#39; sat&#39;, &#39; rat&#39;, &#39;plat&#39;] . re.findall(r&quot; S+at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39;fat&#39;, &#39;cat&#39;, &#39;sat&#39;, &#39;rat&#39;, &#39;splat&#39;] . In case one or more non whitespace that end with &#39;at&#39; are matched. . Starts with and ends with . ^ : Starts with , $ : ends with . re.findall(r&#39; d$&#39;, &quot;This ends with a number 2&quot;) . [&#39;2&#39;] . re.findall(r&#39;^ d&#39;, &quot;5 is the number of choice&quot;) . [&#39;5&#39;] . Exclusion . Square brackerts[^] are used for exclude a character. . phrase = &quot;there are 3 numbers 34 insides 5 this sentence.&quot; . re.findall(r&#39;[^ d]+&#39;, phrase) . [&#39;there are &#39;, &#39; numbers &#39;, &#39; insides &#39;, &#39; this sentence.&#39;] . Removing the punctuation . test_phrase = &#39;This is a string! But it has punctuation. How can we remove it?&#39; . test_phrase . &#39;This is a string! But it has punctuation. How can we remove it?&#39; . re.findall(r&#39;[^!.? ]+&#39;, test_phrase) . [&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;string&#39;, &#39;But&#39;, &#39;it&#39;, &#39;has&#39;, &#39;punctuation&#39;, &#39;How&#39;, &#39;can&#39;, &#39;we&#39;, &#39;remove&#39;, &#39;it&#39;] . Putting it together . clean = &#39; &#39;.join(re.findall(r&#39;[^!.? ]+&#39;, test_phrase)) . clean . &#39;This is a string But it has punctuation How can we remove it&#39; . . text3 = &#39;Only find the hypen-words in this sentence. But you do not know how long-ish they are&#39; . text3 . &#39;Only find the hypen-words in this sentence. But you do not know how long-ish they are&#39; . re.findall(r&#39;[ w]+-[ w+&#39;,text3) . [&#39;hypen-words&#39;, &#39;long-ish&#39;] . Note Difference between [], () . The [] construct in a regex is essentially shorthand for an | on all of the contents. For example [abc] matches a, b or c. Additionally the - character has special meaning inside of a []. It provides a range construct. The regex [a-z] will match any letter a through z. . The () construct is a grouping construct establishing a precedence order (it also has impact on accessing matched substrings but that&#39;s a bit more of an advanced topic). The regex (abc) will match the string &quot;abc&quot;. . . text = &#39;Hello, would you like some catfish?&#39; texttwo = &quot;Hello, would you like to take a catnap?&quot; textthree = &quot;Hello, have you seen this caterpillar?&quot; . re.search(r&#39;cat(fish|nap|claw)&#39;,text).group() . &#39;catfish&#39; . re.search(r&#39;cat(fish|nap|claw)&#39;,texttwo).group() . &#39;catnap&#39; . re.search(r&#39;cat(fish|nap|claw)&#39;,textthree) . &lt;/div&gt; .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/regular-expression/re/text-cleaning/re.findall/exclusion/nlp-chapter-1/2022/06/30/1-4-Regular-Expressions.html",
            "relUrl": "/regular-expression/re/text-cleaning/re.findall/exclusion/nlp-chapter-1/2022/06/30/1-4-Regular-Expressions.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "1.3 Working with pdf files",
            "content": "Installing the library PyPDF2 . pip install PyPDF2 . Collecting PyPDF2 Downloading PyPDF2-2.4.0-py3-none-any.whl (197 kB) Requirement already satisfied: typing-extensions in c: users vicky.crasto anaconda3 lib site-packages (from PyPDF2) (3.10.0.2) Installing collected packages: PyPDF2 Successfully installed PyPDF2-2.4.0 Note: you may need to restart the kernel to use updated packages. . import PyPDF2 . Working with PyPDF2 . Reading a pdf . pdf1 = open(&quot;data_files/US_Declaration.pdf&quot;, &#39;rb&#39;) . Note : the mode is &#39;rb&#39; - read the file as a binary . Creating a pdf reader instance . pdf_reader = PyPDF2.PdfFileReader(pdf1) . Number of pages in the pdf . pdf_reader.numPages . 5 . Extracting text from a page . page_one = pdf_reader.getPage(0) page_one_text = page_one.extractText() page_one_text . &#34; Declaration of Independence nIN CONGRESS, July 4, 1776. nThe unanimous Declaration of the thirteen united States of America, nWhen in the Course of human events, it becomes necessary for one people to dissolve the npolitical bands which have connected them with another, and to assume among the powers of the nearth, the separate and equal station to which the Laws of Nature and of Nature&#39;s God entitle nthem, a decent respect to the opinions of mankind requires that they should declare the causes nwhich impel them to the separation. nWe hold these truths to be self-evident, that all men are created equal, that they are endowed by ntheir Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit nof Happiness.— x14That to secure these rights, Governments are instituted among Men, deriving ntheir just powers from the consent of the governed,— x14That whenever any Form of Government nbecomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to ninstitute new Government, laying its foundation on such principles and organizing its powers in nsuch form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, nindeed, will dictate that Governments long established should not be changed for light and ntransient causes; and accordingly all experience hath shewn, that mankind are more disposed to nsuffer, while evils are sufferable, than to right themselves by abolishing the forms to which they nare accustomed. But when a long train of abuses and usurpations, pursuing invariably the same nObject evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, nto throw off such Government, and to provide new Guards for their future security.— x14Such has nbeen the patient sufferance of these Colonies; and such is now the necessity which constrains nthem to alter their former Systems of Government. The history of the present King of Great nBritain is a history of repeated injuries and usurpations, all having in direct object the nestablishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a ncandid world. nHe has refused his Assent to Laws, the most wholesome and necessary for the npublic good. nHe has forbidden his Governors to pass Laws of immediate and pressing nimportance, unless suspended in their operation till his Assent should be obtained; nand when so suspended, he has utterly neglected to attend to them. nHe has refused to pass other Laws for the accommodation of large districts of npeople, unless those people would relinquish the right of Representation in the nLegislature, a right inestimable to them and formidable to tyrants only. nHe has called together legislative bodies at places unusual, uncomfortable, and distant nfrom the depository of their public Records, for the sole purpose of fatiguing them into ncompliance with his measures.&#34; . pdf1.close() . Adding pages to pdf file . Open the pdf and extracting the first page . pdf2 = open(&quot;data_files/US_Declaration.pdf&quot;,&#39;rb&#39;) pdf_reader = PyPDF2.PdfFileReader(pdf2) first_page = pdf_reader.getPage(0) . Creating a writer object . pdf_writer = PyPDF2.PdfFileWriter() pdf_writer.addPage(first_page) . pdf_output = open(&quot;New_doc.pdf&quot;, &#39;wb&#39;) . pdf_writer.write(pdf_output) pdf_output.close() pdf2.close() . Checking the new doc which is created . pdf3 = open(&quot;New_doc.pdf&quot;, &#39;rb&#39;) pdf_reader = PyPDF2.PdfFileReader(pdf3) pdf_reader.numPages . 1 . pdf3.close() .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/pypdf2/pdf/nlp-chapter-1/2022/06/29/1-3-Working-with-pdf-files.html",
            "relUrl": "/pypdf2/pdf/nlp-chapter-1/2022/06/29/1-3-Working-with-pdf-files.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "1.2 Working with text  files",
            "content": "Working with text file in python . Creating a text file in python . %%writefile test.txt ( Use this magic command before the text) . Hello, this is a new file create using python ide. This the second line of the file. . Writing test.txt . Understanding the location of the file . Give the location of the present working directory . pwd() . &#39;C: Users Vicky.Crasto OneDrive - Unilever Work_file_082021 05_Other_learning NLP 01_Udemy_JoseP&#39; . Opening the file . myfile = open(&quot;test.txt&quot;) . myfile . &lt;_io.TextIOWrapper name=&#39;test.txt&#39; mode=&#39;r&#39; encoding=&#39;cp1252&#39;&gt; . This is the location in the memory which hold the file. . Using .read() and .seek() . myfile.read() . &#39;Hello, this is a new file create using python ide. nThis the second line of the file. n&#39; . myfile.read() . &#39;&#39; . The second time the function it called it does not give any output since the cursor has reached the end of the document. There is nothing more read. Hence we need to reset the cursor to the start. . Resetting the cursor . myfile.seek(0) . 0 . myfile.read() . &#39;Hello, this is a new file create using python ide. nThis the second line of the file. n&#39; . Using .readlines() . readlines() help to read the file line by line. Note: All the data is helded in the memory, hence large files will need to handled carefully. . myfile.seek(0) myfile.readlines() . [&#39;Hello, this is a new file create using python ide. n&#39;, &#39;This the second line of the file. n&#39;] . myfile.close() . Writing a file - Understanding the mode . While opening the file, we can open it with different modes . &#39;r&#39; default to read the file | &#39;w+&#39; read and write the file.(Overwrites the existing file) | &#39;wb+&#39; read and write as binary (used in case of pdf) | . myfile = open(&quot;test.txt&quot;, mode= &#39;w+&#39;) myfile.write(&quot;This is an additional file&quot;) . 26 . myfile.seek(0) myfile.readlines() . [&#39;This is an additional file&#39;] . Hence the existing data is deleted and the new data is overwrite. . myfile.close() . Appending a file . Passing the argument &#39;a&#39; opens the file and puts the pointer at the end, so anything written is appended. . myfile = open(&quot;test.txt&quot;, &#39;a+&#39;) myfile.write(&quot; nAppending a new line to the existing line&quot;) myfile.seek(0) print(myfile.read()) . This is an additional file Appending a new line to the existing line Appending a new line to the existing line Appending a new line to the existing line Appending a new line to the existing line . myfile.close() . Aliases and context managers . You can assign temporary variable names as aliases, and manage the opening and closing of files automatically using a context manager: . with open(&#39;test.txt&#39;,&#39;r&#39;) as txt: first_line = txt.readlines()[0] print(first_line) . This is an additional file . By using this method, the file is opened, read and closed by context mananger automatically after doing the specified operation. . first_line . &#39;This is an additional file n&#39; . txt.read() . ValueError Traceback (most recent call last) C: Users VICKY~1.CRA AppData Local Temp/ipykernel_9856/1416744708.py in &lt;module&gt; -&gt; 1 txt.read() ValueError: I/O operation on closed file. . Hence the extract line remain in the object but the file is closed by the context manager. . Iterating through a file . with open(&#39;test.txt&#39;, &#39;r&#39;) as txt: for line in txt: print(line , end=&#39;$$$$&#39;) . This is an additional file $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line$$$$ .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/readlines()/textfiles/nlp-chapter-1/2022/06/28/1-2-Working-with-text-files.html",
            "relUrl": "/readlines()/textfiles/nlp-chapter-1/2022/06/28/1-2-Working-with-text-files.html",
            "date": " • Jun 28, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "1.1 Learning to use F string literal",
            "content": "Topics covered . F string formating in printing text | Alignment, padding Fstring | Format dates in Printing | . F string basics . person = &quot;Richard&quot; . print(f&quot;The name of the boy is {person}&quot;) . The name of the boy is Richard . Using a dictionary with f string . d = {&#39;Roll no&#39; : 12 , &#39;Subject&#39;: &quot;English&quot;} . print(f&quot;The student with roll no {d[&#39;Roll no&#39;]}, got highest marks in {d[&#39;Subject&#39;]}&quot;) . The student with roll no 12, got highest marks in English . Using a list . l = [&quot;mango&quot;, &quot;orange&quot;,&quot;banana&quot;] . print(f&quot;The fruit that I enjoy the most is {l[0]} and {l[1]}&quot;) . The fruit that I enjoy the most is mango and orange . Minimum Widths, Alignment and Padding . You can pass arguments inside a nested set of curly braces to set a minimum width for the field, the alignment and even padding characters. . library = [(&#39;Author&#39;, &#39;Topic&#39;, &#39;Pages&#39;), (&#39;Twain&#39;, &#39;Rafting&#39;, 601), (&#39;Feynman&#39;, &#39;Physics&#39;, 95), (&#39;Hamilton&#39;, &#39;Mythology&#39;, 144)] . Tuple unpacking . for author, topic, page in library: print(f&quot;{author}, {topic},{page}&quot;) . Author, Topic,Pages Twain, Rafting,601 Feynman, Physics,95 Hamilton, Mythology,144 . aligning the text . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{8}}{page:{7}}&quot;) . Author Topic Pages Twain Rafting 601 Feynman Physics 95 Hamilton Mythology 144 . Here the first three lines align, except Pages follows a default left-alignment while numbers are right-aligned. Also, the fourth line&#39;s page number is pushed to the right as Mythology exceeds the minimum field width of 8. When setting minimum field widths make sure to take the longest item into account. . To set the alignment, use the character &lt; for left-align, ^ for center, &gt; for right. To set padding, precede the alignment character with the padding character (- and . are common choices). . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{10}}{page:&gt;{7}}&quot;) . Author Topic Pages Twain Rafting 601 Feynman Physics 95 Hamilton Mythology 144 . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{10}}{page:.&gt;{7}}&quot;) . Author Topic ..Pages Twain Rafting ....601 Feynman Physics .....95 Hamilton Mythology ....144 . . from datetime import datetime . today = datetime(year=2022, month =1, day = 27) . print(f&#39;{today:%B,%d, %Y}&#39;) . January,27, 2018 .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/nlp/nlp-chapter-1/2022/06/25/1-1-Text-Basics-using-Fstring-literal.html",
            "relUrl": "/nlp/nlp-chapter-1/2022/06/25/1-1-Text-Basics-using-Fstring-literal.html",
            "date": " • Jun 25, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vicky-crasto.github.io/Learn-Project-Language/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page7": {
          "title": "An Example Markdown Post",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . {% include alert.html text=”You can include alert boxes” %} . …and… . {% include info.html text=”You can include info boxes” %} . Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . {% twitter https://twitter.com/jakevdp/status/1204765621767901185?s=20 %} . Footnotes . This is the footnote. &#8617; . |",
          "url": "https://vicky-crasto.github.io/Learn-Project-Language/extras/test.html",
          "relUrl": "/extras/test.html",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vicky-crasto.github.io/Learn-Project-Language/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}