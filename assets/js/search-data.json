{
  
    
        "post0": {
            "title": "2.2 Tokenization",
            "content": "The first step of creating Doc object is breakdown the raw text into smaller pieces or tokens . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . mystring = &#39;&quot;We &#39;re moving to L.A.!&quot;&#39; print(mystring) . &#34;We&#39;re moving to L.A.!&#34; . doc = nlp(mystring) for token in doc: print(token.text, end = &quot; | &quot;) . &#34; | We | &#39;re | moving | to | L.A. | ! | &#34; | . Spacy follows the following sequence to break the text. . . Prefix: Character(s) at the beginning &#9656; $ ( “ ¿ | Suffix: Character(s) at the end &#9656; km ) , . ! ” | Infix: Character(s) in between &#9656; - -- / ... | Exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied &#9656; `St. U.S. | . Tokens are the basic building blocks of a Doc object - everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another. . Prefixes, Suffixes and Infixes . spaCy will isolate punctuation that does not form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token. . doc2 = nlp(u&quot;We&#39;re here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!&quot;) for t in doc2: print(t) . We &#39;re here to help ! Send snail - mail , email support@oursite.com or visit us at http://www.oursite.com ! . Note - We see that dash, !, commas are assigned as seperate tokens. But email address and website and kept together. . doc3 = nlp(u&#39;A 5km NYC cab ride costs $10.30&#39;) for t in doc3: print(t) . A 5 km NYC cab ride costs $ 10.30 . Hence the dollar sign and amount are given seperate tokens. . Exceptions . Punctuation that exists as part of a known abbreviation will be kept as part of the token. . doc4 = nlp(u&quot;Let&#39;s visit St. Louis in the U.S. next year.&quot;) for t in doc4: print(t) . Let &#39;s visit St. Louis in the U.S. next year . . In this case we see abbrevation such St. and U.S are preseved in seperate tokens. . Counting tokens . len(doc) . 8 . Counting Vocab Entrmies . len(doc.vocab) . 802 . Retriving tokens by index position and slice . doc5 = nlp(u&#39;It is better to give than to receive.&#39;) ### Retrieve the third token: doc5[2] . better . doc5[2:5] . better to give . . doc6 = nlp(u&#39;My dinner was horrible.&#39;) doc7 = nlp(u&#39;Your dinner was delicious.&#39;) . doc6[3] = doc7[3] . TypeError Traceback (most recent call last) C: Users VICKY~1.CRA AppData Local Temp/ipykernel_11844/3591796456.py in &lt;module&gt; -&gt; 1 doc6[3] = doc7[3] TypeError: &#39;spacy.tokens.doc.Doc&#39; object does not support item assignment . In this case we are trying to replace horrible in doc6 with delicious from doc7. But it cannot be done. . Named Entities . The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the ents property of a Doc object. . doc8 = nlp(u&#39;Apple to build a Hong Kong factory for $6 million&#39;) for token in doc8: print(token.text, end=&#39; | &#39;) print(&#39; n-&#39;) for ent in doc8.ents: print(ent.text+&#39; - &#39;+ent.label_+&#39; - &#39;+str(spacy.explain(ent.label_))) . Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | - Hong Kong - GPE - Countries, cities, states $6 million - MONEY - Monetary values, including unit . . Doc.noun_chunks are another object property. Noun chunks are &quot;base noun phrases&quot; – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in Sheb Wooley&#39;s 1958 song, a &quot;one-eyed, one-horned, flying, purple people-eater&quot; would be one long noun chunk. . doc9 = nlp(u&quot;Autonomous cars shift insurance liability toward manufacturers.&quot;) for chunk in doc9.noun_chunks: print(chunk.text) . Autonomous cars insurance liability manufacturers . doc10 = nlp(u&quot;Red cars do not carry higher insurance rates.&quot;) for chunk in doc10.noun_chunks: print(chunk.text) . Red cars higher insurance rates . doc11 = nlp(u&quot;He was a one-eyed, one-horned, flying, purple people-eater.&quot;) for chunk in doc11.noun_chunks: print(chunk.text) . He purple people-eater . Built in Visualizers . spaCy includes a built-in visualization tool called displaCy . from spacy import displacy doc = nlp(u&#39;Apple is going to build a U.K. factory for $6 million.&#39;) displacy.render(doc, style=&#39;dep&#39;, jupyter=True, options={&#39;distance&#39;: 120}) . Apple PROPN is AUX going VERB to PART build VERB a DET U.K. PROPN factory NOUN for ADP $ SYM 6 NUM million. NUM nsubj aux aux xcomp det compound dobj prep quantmod compound pobj The optional &#39;distance&#39; argument sets the distance between tokens. If the distance is made too small, text that appears beneath short arrows may become too compressed to read. . Visualizing the entity recognizer . doc = nlp(u&#39;Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.&#39;) displacy.render(doc, style=&#39;ent&#39;, jupyter=True) . Over the last quarter DATE Apple ORG sold nearly 20 thousand CARDINAL iPods PRODUCT for a profit of $6 million MONEY .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/nlp-chapter-2/2022/07/01/2-2-Tokenization.html",
            "relUrl": "/spacy/nlp-chapter-2/2022/07/01/2-2-Tokenization.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2.1 Spacy Basics",
            "content": "Installation and setup . pip install -U spacy . Downloading spacy vocab library . !python -m spacy download en . Loading Spacy . import spacy nlp = spacy.load(&#39;en_core_web_sm&#39;) . Creating a doc object and printing the different components of the token . ### Create a doc object doc = nlp(u&#39;Tesla is looking at buying U.S. startup for $6 million&#39;) ### Printing each token seperately for token in doc: print(f&#39;{token.text:&gt;{10}} {token.pos_:&gt;{10}} {token.dep_:&gt;{10}}&#39;) . Tesla NOUN nsubj is AUX aux looking VERB ROOT at ADP prep buying VERB pcomp U.S. PROPN compound startup NOUN dobj for ADP prep $ SYM quantmod 6 NUM compound million NUM pobj . Understanding the spacy pipeline . When we run nlp, our text enters a processing pipeline that first breaks down the text and then performs a series of operations to tag, parse and describe the data. Image source: https://spacy.io/usage/spacy-101#pipelines . . NAME COMPONENT CREATES DESCRIPTION . tokenizer | Tokenizer | Doc | Segment text into tokens. | . tagger | Tagger | Token.tag | Assign part-of-speech tags. | . parser | DependencyParser | Token.head,&nbsp;Token.dep,&nbsp;Doc.sents,&nbsp;Doc.noun_chunks | Assign dependency labels. | . ner | EntityRecognizer | Doc.ents,&nbsp;Token.ent_iob,&nbsp;Token.ent_type | Detect and label named entities. | . lemmatizer | Lemmatizer | Token.lemma | Assign base forms. | . textcat | TextCategorizer | Doc.cats | Assign document labels. | . custom | custom components | Doc._.xxx,&nbsp;Token._.xxx,&nbsp;Span._.xxx | Assign custom attributes, methods or properties. | . nlp.pipeline . [(&#39;tok2vec&#39;, &lt;spacy.pipeline.tok2vec.Tok2Vec at 0x23264bfa220&gt;), (&#39;tagger&#39;, &lt;spacy.pipeline.tagger.Tagger at 0x23264bfae80&gt;), (&#39;parser&#39;, &lt;spacy.pipeline.dep_parser.DependencyParser at 0x23264a554a0&gt;), (&#39;attribute_ruler&#39;, &lt;spacy.pipeline.attributeruler.AttributeRuler at 0x23264cb5780&gt;), (&#39;lemmatizer&#39;, &lt;spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x23264cc8780&gt;), (&#39;ner&#39;, &lt;spacy.pipeline.ner.EntityRecognizer at 0x23264a55350&gt;)] . nlp.pipe_names . [&#39;tok2vec&#39;, &#39;tagger&#39;, &#39;parser&#39;, &#39;attribute_ruler&#39;, &#39;lemmatizer&#39;, &#39;ner&#39;] . Tokenization . Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words. . Source: https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4s . doc2 = nlp(u&quot;Apple isn&#39;t looking into buying startups.&quot;) for token in doc2: print(f&#39;{token.text:&gt;{10}} {token.pos_:&gt;{10}} {token.dep_:&gt;{10}}&#39;) . Apple PROPN nsubj is AUX aux n&#39;t PART neg SPACE dep looking VERB ROOT into ADP prep buying VERB pcomp startups NOUN dobj . PUNCT punct . Things to take note . Spacy is able to recognize the root verb and the negation, hence it has split is&#39;nt into two tokens | Spaces and peroid are assigned as tokens | . type(doc2) . spacy.tokens.doc.Doc . doc2 is spacy object and contains information of each token in the text. . Part of Speech Tagging(POS) . In the above example we see the output has clearly label Apple as pronoun, looking as a verb etc.These are parts of speech. . For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging . Dependencies . We also looked at the syntactic dependencies assigned to each token. Tesla is identified as an nsubj or the nominal subject of the sentence. . For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing A good explanation of typed dependencies can be found here . Full name of a tag used in spacy . spacy.explain(&#39;PROPN&#39;) . &#39;proper noun&#39; . spacy.explain(&#39;nsubj&#39;) . &#39;nominal subject&#39; . Additional Token Attributes . Tag Description doc2[0].tag . .text | The original word text | Tesla | . .lemma_ | The base form of the word | tesla | . .pos_ | The simple part-of-speech tag | PROPN/proper noun | . .tag_ | The detailed part-of-speech tag | NNP/noun, proper singular | . .shape_ | The word shape – capitalization, punctuation, digits | Xxxxx | . .is_alpha | Is the token an alpha character? | True | . .is_stop | Is the token part of a stop list, i.e. the most common words of the language? | False | . print(doc2[4].text) print(doc2[4].lemma_) . looking look . print(doc2[0].text + &#39;: &#39; + doc2[0].shape_) print(doc[5].text + &#39;: &#39; + doc[5].shape_) . Apple: Xxxxx U.S.: X.X. . . Large Doc objects can be hard to work with at times. A span is a slice of Doc object in the form Doc[start:stop]. . doc3 = nlp(u&#39;Although commmonly attributed to John Lennon from his song &quot;Beautiful Boy&quot;, the phrase &quot;Life is what happens to us while we are making other plans&quot; was written by cartoonist Allen Saunders and published in Reader &#39;s Digest in 1957, when Lennon was 17.&#39;) . life_qoute = doc3[16:30] print(life_qoute) . &#34;Life is what happens to us while we are making other plans&#34; . type(life_qoute) . spacy.tokens.span.Span . . Certain tokens inside a Doc object may also receive a &quot;start of sentence&quot; tag. While this doesn&#39;t immediately build a list of sentences, these tags enable the generation of sentence segments through Doc.sents. . doc4 = nlp(u&#39;This is the first sentence. This is another sentence. This is the last sentence.&#39;) . for sent in doc4.sents: print(sent) . This is the first sentence. This is another sentence. This is the last sentence. . doc4[6].is_sent_start . True .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/spacy/nlp-chapter-2/2022/07/01/2-1-Spacy-Basics.html",
            "relUrl": "/spacy/nlp-chapter-2/2022/07/01/2-1-Spacy-Basics.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "1.5 Text Basics - Practice exercises",
            "content": "f-Strings . 1. Print an f-string that displays NLP stands for Natural Language Processing using the variables provided. . abbr = &#39;NLP&#39; full_text = &#39;Natural Language Processing&#39; ### Enter your code here: . NLP stands for Natural Language Processing . Solution . abbr = &#39;NLP&#39; full_text = &#39;Natural Language Processing&#39; print(f&#39;{abbr} stands for {full_text}&#39;) . NLP stands for Natural Language Processing . Files . 2. Create a file in the current working directory called contacts.txt by running the cell below: . %%writefile contacts.txt First_Name Last_Name, Title, Extension, Email . Overwriting contacts.txt . Solution . %%writefile contacts.txt First_Name Last_Name, Title, Extension, Email . Overwriting contacts.txt . 3. Open the file and use .read() to save the contents of the file to a string called fields. Make sure the file is closed at the end. . ### Write your code here: ### Run fields to see the contents of contacts.txt: fields . &#39;First_Name Last_Name, Title, Extension, Email&#39; . Solution . with open(&quot;contacts.txt&quot;) as text: fields = text.read() fields . &#39;First_Name Last_Name, Title, Extension, Email n&#39; . Working with PDF Files . 4. Use PyPDF2 to open the file Business_Proposal.pdf. Extract the text of page 2. . # Open the file as a binary object # Use PyPDF2 to read the text of the file # Get the text from page 2 (CHALLENGE: Do this in one step!) page_two_text = # Close the file # Print the contents of page_two_text print(page_two_text) . AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . Solution . import PyPDF2 # Open the file as a binary object pdf1 = open(&quot;data_files/Business_Proposal.pdf&quot;, &#39;rb&#39;) # Use PyPDF2 to read the text of the file pdf_reader = PyPDF2.PdfFileReader(pdf1) # Get the text from page 2 (CHALLENGE: Do this in one step!) page_two_text = pdf_reader.getPage(1).extractText() # Close the file pdf1.close() # Print the contents of page_two_text print(page_two_text) . AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . import re re.findall(r&#39;[^(AUTHORS:)]&#39;, page_two_text) . [&#39; n&#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;m&#39;, &#39;y&#39;, &#39; &#39;, &#39;B&#39;, &#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;r&#39;, &#39;,&#39;, &#39; &#39;, &#39;F&#39;, &#39;i&#39;, &#39;n&#39;, &#39;a&#39;, &#39;n&#39;, &#39;c&#39;, &#39;e&#39;, &#39; &#39;, &#39;C&#39;, &#39;h&#39;, &#39;a&#39;, &#39;i&#39;, &#39;r&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;,&#39;, &#39; &#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;r&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;C&#39;, &#39;h&#39;, &#39;r&#39;, &#39;i&#39;, &#39;s&#39;, &#39; &#39;, &#39;D&#39;, &#39;o&#39;, &#39;n&#39;, &#39;a&#39;, &#39;l&#39;, &#39;d&#39;, &#39;s&#39;, &#39;o&#39;, &#39;n&#39;, &#39;,&#39;, &#39; &#39;, &#39;c&#39;, &#39;c&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;t&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39; &#39;, &#39;D&#39;, &#39;i&#39;, &#39;r&#39;, &#39;.&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;6&#39;, &#39;2&#39;, &#39;1&#39;, &#39;,&#39;, &#39; &#39;, &#39;c&#39;, &#39;d&#39;, &#39;o&#39;, &#39;n&#39;, &#39;a&#39;, &#39;l&#39;, &#39;d&#39;, &#39;s&#39;, &#39;o&#39;, &#39;n&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;, &#39; n&#39;, &#39; n&#39;, &#39;E&#39;, &#39;r&#39;, &#39;i&#39;, &#39;n&#39;, &#39; &#39;, &#39;F&#39;, &#39;r&#39;, &#39;e&#39;, &#39;e&#39;, &#39;m&#39;, &#39;a&#39;, &#39;n&#39;, &#39;,&#39;, &#39; &#39;, &#39;r&#39;, &#39;.&#39;, &#39; &#39;, &#39;V&#39;, &#39;P&#39;, &#39;,&#39;, &#39; &#39;, &#39;x&#39;, &#39;8&#39;, &#39;7&#39;, &#39;9&#39;, &#39;,&#39;, &#39; &#39;, &#39;e&#39;, &#39;f&#39;, &#39;r&#39;, &#39;e&#39;, &#39;e&#39;, &#39;m&#39;, &#39;a&#39;, &#39;n&#39;, &#39;@&#39;, &#39;o&#39;, &#39;u&#39;, &#39;r&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;p&#39;, &#39;a&#39;, &#39;n&#39;, &#39;y&#39;, &#39;.&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39; n&#39;, &#39; &#39;, &#39; &#39;] . 5. Open the file contacts.txt in append mode. Add the text of page 2 from above to contacts.txt. . CHALLENGE: See if you can remove the word &quot;AUTHORS:&quot; . . First_Name Last_Name, Title, Extension, EmailAUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . **Simple Solution** . myfile = open(&#39;contacts.txt&#39;, &#39;a+&#39;) myfile.seek(0) print(myfile.read()) . First_Name Last_Name, Title, Extension, Email . myfile.write(page_two_text) myfile.seek(0) print(myfile.read()) . First_Name Last_Name, Title, Extension, Email AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . myfile.close() . . First_Name Last_Name, Title, Extension, Email Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . with open(&#39;contacts.txt&#39;,&#39;a+&#39;) as c: c.write(page_two_text[8:]) c.seek(0) print(c.read()) . First_Name Last_Name, Title, Extension, Email AUTHORS: Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com Amy Baker, Finance Chair, x345, abaker@ourcompany.com Chris Donaldson, Accounting Dir., x621, cdonaldson@ourcompany.com Erin Freeman, Sr. VP, x879, efreeman@ourcompany.com . Regular Expressions . 6. Using the page_two_text variable created above, extract any email addresses that were contained in the file Business_Proposal.pdf. . import re # Enter your regex pattern here. This may take several tries! pattern = re.findall(pattern, page_two_text) . [&#39;abaker@ourcompany.com&#39;, &#39;cdonaldson@ourcompany.com&#39;, &#39;efreeman@ourcompany.com&#39;] . Solution . import re pattern = r&#39; w+@ w+.com&#39; re.findall(pattern, page_two_text) . [&#39;abaker@ourcompany.com&#39;, &#39;cdonaldson@ourcompany.com&#39;, &#39;efreeman@ourcompany.com&#39;] .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/pypdf2/pdf/nlp-chapter-1/2022/06/30/1-5-Python-Text-Basics-Practice.html",
            "relUrl": "/pypdf2/pdf/nlp-chapter-1/2022/06/30/1-5-Python-Text-Basics-Practice.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "1.4 Regular Expression",
            "content": "Regular exp. library . import re . Finding the first instance in the text . text = &quot;The phone number given in the helpline is 408-999-4567&quot; pattern = &#39;phone&#39; re.search(pattern, text) . &lt;re.Match object; span=(4, 9), match=&#39;phone&#39;&gt; . If the match is found then search return the location of the match. Note: It only gives the first instance in the text. . Span is the starting and ending index of the match. (Index starts from zero) . match=re.search(pattern, text) match . &lt;re.Match object; span=(4, 9), match=&#39;phone&#39;&gt; . .span() give the span of the match, .start() give the start index, .end() gives the end index . match.span() . (4, 9) . match.start() . 4 . match.end() . 9 . Find all instances in the text . text1 = &quot;My phone is a hi-tech phone. The phone is dual band, with the lastest phone-tech processor&quot; . matches = re.findall(&quot;phone&quot;, text1) matches . [&#39;phone&#39;, &#39;phone&#39;, &#39;phone&#39;, &#39;phone&#39;] . len(matches) . 4 . . for match in re.finditer(&#39;phone&#39;, text1): print(match.span()) . (3, 8) (22, 27) (33, 38) (70, 75) . To find the word matched, use .group() method . match.group() . &#39;phone&#39; . Identifiers in Regex . CharacterDescriptionExample Pattern CodeExammple Match . &lt;/p&gt; d | A digit | file_ d d | file_25 | . w | Alphanumeric | w- w w w | A-b_1 | . s | White space | a sb sc | a b c | . D | A non digit | D D D | ABC | . W | Non-alphanumeric | W W W W W | *-+=) | . S | Non-whitespace | S S S S | Yoyo | . &lt;/table&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; text . &#39;The phone number given in the helpline is 408-999-4567&#39; . If we want to find phone number with the pattern xxx-xxx-xxxx, we can use the identifier for it. . re.search(r&#39; d d d- d d d- d d d d&#39;, text).group() . &#39;408-999-4567&#39; . Quantifiers in Regex . In repeating the identifier, we can use quantifiers to do the same thing. . CharacterDescriptionExample Pattern CodeExammple Match . &lt;/p&gt; + | Occurs one or more times | Version w- w+ | Version A-b1_1 | . {3} | Occurs exactly 3 times | D{3} | abc | . {2,4} | Occurs 2 to 4 times | d{2,4} | 123 | . {3,} | Occurs 3 or more | w{3,} | anycharacters | . * | Occurs zero or more times | A *B *C* | AAACC | . ? | Once or none | plurals? | plural | . &lt;/table&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; re.search(r&#39; d{3}- d{3}- d{4}&#39;, text).group() . &#39;408-999-4567&#39; . Groups in Regex search . Using parentheses in regex we can create groups with the matched data . phone_pattern = re.compile(r&#39;( d{3})-( d{3})-( d{4})&#39;) . results = re.search(phone_pattern, text) . results.group() . &#39;408-999-4567&#39; . Each parentheses in the regex pattern is group which can called out. . results.group(1) . &#39;408&#39; . results.group(2) . &#39;999&#39; . results.group(3) . &#39;4567&#39; . Or operator | . re.search(r&quot;man|woman&quot;, &quot;This man is a good person&quot;) . &lt;re.Match object; span=(5, 8), match=&#39;man&#39;&gt; . re.search(r&quot;man|woman&quot;, &quot;This woman is a good person&quot;) . &lt;re.Match object; span=(5, 10), match=&#39;woman&#39;&gt; . Wildcard characters . re.findall(r&quot;.at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39;fat&#39;, &#39;cat&#39;, &#39; at&#39;, &#39;sat&#39;, &#39;rat&#39;, &#39;lat&#39;] . We see that all 3 letter word being matched. One single period matches on wildcard letter before the pattern. . re.findall(r&quot;..at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39; fat&#39;, &#39; cat&#39;, &#39; sat&#39;, &#39; rat&#39;, &#39;plat&#39;] . re.findall(r&quot; S+at&quot;, &quot;The fat cat ate the peta bread and sat on the rattop and splat&quot;) . [&#39;fat&#39;, &#39;cat&#39;, &#39;sat&#39;, &#39;rat&#39;, &#39;splat&#39;] . In case one or more non whitespace that end with &#39;at&#39; are matched. . Starts with and ends with . ^ : Starts with , $ : ends with . re.findall(r&#39; d$&#39;, &quot;This ends with a number 2&quot;) . [&#39;2&#39;] . re.findall(r&#39;^ d&#39;, &quot;5 is the number of choice&quot;) . [&#39;5&#39;] . Exclusion . Square brackerts[^] are used for exclude a character. . phrase = &quot;there are 3 numbers 34 insides 5 this sentence.&quot; . re.findall(r&#39;[^ d]+&#39;, phrase) . [&#39;there are &#39;, &#39; numbers &#39;, &#39; insides &#39;, &#39; this sentence.&#39;] . Removing the punctuation . test_phrase = &#39;This is a string! But it has punctuation. How can we remove it?&#39; . test_phrase . &#39;This is a string! But it has punctuation. How can we remove it?&#39; . re.findall(r&#39;[^!.? ]+&#39;, test_phrase) . [&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;string&#39;, &#39;But&#39;, &#39;it&#39;, &#39;has&#39;, &#39;punctuation&#39;, &#39;How&#39;, &#39;can&#39;, &#39;we&#39;, &#39;remove&#39;, &#39;it&#39;] . Putting it together . clean = &#39; &#39;.join(re.findall(r&#39;[^!.? ]+&#39;, test_phrase)) . clean . &#39;This is a string But it has punctuation How can we remove it&#39; . . text3 = &#39;Only find the hypen-words in this sentence. But you do not know how long-ish they are&#39; . text3 . &#39;Only find the hypen-words in this sentence. But you do not know how long-ish they are&#39; . re.findall(r&#39;[ w]+-[ w+&#39;,text3) . [&#39;hypen-words&#39;, &#39;long-ish&#39;] . Note Difference between [], () . The [] construct in a regex is essentially shorthand for an | on all of the contents. For example [abc] matches a, b or c. Additionally the - character has special meaning inside of a []. It provides a range construct. The regex [a-z] will match any letter a through z. . The () construct is a grouping construct establishing a precedence order (it also has impact on accessing matched substrings but that&#39;s a bit more of an advanced topic). The regex (abc) will match the string &quot;abc&quot;. . . text = &#39;Hello, would you like some catfish?&#39; texttwo = &quot;Hello, would you like to take a catnap?&quot; textthree = &quot;Hello, have you seen this caterpillar?&quot; . re.search(r&#39;cat(fish|nap|claw)&#39;,text).group() . &#39;catfish&#39; . re.search(r&#39;cat(fish|nap|claw)&#39;,texttwo).group() . &#39;catnap&#39; . re.search(r&#39;cat(fish|nap|claw)&#39;,textthree) . &lt;/div&gt; .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/regular-expression/re/text-cleaning/re.findall/exclusion/nlp-chapter-1/2022/06/30/1-4-Regular-Expressions.html",
            "relUrl": "/regular-expression/re/text-cleaning/re.findall/exclusion/nlp-chapter-1/2022/06/30/1-4-Regular-Expressions.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "1.3 Working with pdf files",
            "content": "Installing the library PyPDF2 . pip install PyPDF2 . Collecting PyPDF2 Downloading PyPDF2-2.4.0-py3-none-any.whl (197 kB) Requirement already satisfied: typing-extensions in c: users vicky.crasto anaconda3 lib site-packages (from PyPDF2) (3.10.0.2) Installing collected packages: PyPDF2 Successfully installed PyPDF2-2.4.0 Note: you may need to restart the kernel to use updated packages. . import PyPDF2 . Working with PyPDF2 . Reading a pdf . pdf1 = open(&quot;data_files/US_Declaration.pdf&quot;, &#39;rb&#39;) . Note : the mode is &#39;rb&#39; - read the file as a binary . Creating a pdf reader instance . pdf_reader = PyPDF2.PdfFileReader(pdf1) . Number of pages in the pdf . pdf_reader.numPages . 5 . Extracting text from a page . page_one = pdf_reader.getPage(0) page_one_text = page_one.extractText() page_one_text . &#34; Declaration of Independence nIN CONGRESS, July 4, 1776. nThe unanimous Declaration of the thirteen united States of America, nWhen in the Course of human events, it becomes necessary for one people to dissolve the npolitical bands which have connected them with another, and to assume among the powers of the nearth, the separate and equal station to which the Laws of Nature and of Nature&#39;s God entitle nthem, a decent respect to the opinions of mankind requires that they should declare the causes nwhich impel them to the separation. nWe hold these truths to be self-evident, that all men are created equal, that they are endowed by ntheir Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit nof Happiness.— x14That to secure these rights, Governments are instituted among Men, deriving ntheir just powers from the consent of the governed,— x14That whenever any Form of Government nbecomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to ninstitute new Government, laying its foundation on such principles and organizing its powers in nsuch form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, nindeed, will dictate that Governments long established should not be changed for light and ntransient causes; and accordingly all experience hath shewn, that mankind are more disposed to nsuffer, while evils are sufferable, than to right themselves by abolishing the forms to which they nare accustomed. But when a long train of abuses and usurpations, pursuing invariably the same nObject evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, nto throw off such Government, and to provide new Guards for their future security.— x14Such has nbeen the patient sufferance of these Colonies; and such is now the necessity which constrains nthem to alter their former Systems of Government. The history of the present King of Great nBritain is a history of repeated injuries and usurpations, all having in direct object the nestablishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a ncandid world. nHe has refused his Assent to Laws, the most wholesome and necessary for the npublic good. nHe has forbidden his Governors to pass Laws of immediate and pressing nimportance, unless suspended in their operation till his Assent should be obtained; nand when so suspended, he has utterly neglected to attend to them. nHe has refused to pass other Laws for the accommodation of large districts of npeople, unless those people would relinquish the right of Representation in the nLegislature, a right inestimable to them and formidable to tyrants only. nHe has called together legislative bodies at places unusual, uncomfortable, and distant nfrom the depository of their public Records, for the sole purpose of fatiguing them into ncompliance with his measures.&#34; . pdf1.close() . Adding pages to pdf file . Open the pdf and extracting the first page . pdf2 = open(&quot;data_files/US_Declaration.pdf&quot;,&#39;rb&#39;) pdf_reader = PyPDF2.PdfFileReader(pdf2) first_page = pdf_reader.getPage(0) . Creating a writer object . pdf_writer = PyPDF2.PdfFileWriter() pdf_writer.addPage(first_page) . pdf_output = open(&quot;New_doc.pdf&quot;, &#39;wb&#39;) . pdf_writer.write(pdf_output) pdf_output.close() pdf2.close() . Checking the new doc which is created . pdf3 = open(&quot;New_doc.pdf&quot;, &#39;rb&#39;) pdf_reader = PyPDF2.PdfFileReader(pdf3) pdf_reader.numPages . 1 . pdf3.close() .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/pypdf2/pdf/nlp-chapter-1/2022/06/29/1-3-Working-with-pdf-files.html",
            "relUrl": "/pypdf2/pdf/nlp-chapter-1/2022/06/29/1-3-Working-with-pdf-files.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "1.2 Working with text  files",
            "content": "Working with text file in python . Creating a text file in python . %%writefile test.txt ( Use this magic command before the text) . Hello, this is a new file create using python ide. This the second line of the file. . Writing test.txt . Understanding the location of the file . Give the location of the present working directory . pwd() . &#39;C: Users Vicky.Crasto OneDrive - Unilever Work_file_082021 05_Other_learning NLP 01_Udemy_JoseP&#39; . Opening the file . myfile = open(&quot;test.txt&quot;) . myfile . &lt;_io.TextIOWrapper name=&#39;test.txt&#39; mode=&#39;r&#39; encoding=&#39;cp1252&#39;&gt; . This is the location in the memory which hold the file. . Using .read() and .seek() . myfile.read() . &#39;Hello, this is a new file create using python ide. nThis the second line of the file. n&#39; . myfile.read() . &#39;&#39; . The second time the function it called it does not give any output since the cursor has reached the end of the document. There is nothing more read. Hence we need to reset the cursor to the start. . Resetting the cursor . myfile.seek(0) . 0 . myfile.read() . &#39;Hello, this is a new file create using python ide. nThis the second line of the file. n&#39; . Using .readlines() . readlines() help to read the file line by line. Note: All the data is helded in the memory, hence large files will need to handled carefully. . myfile.seek(0) myfile.readlines() . [&#39;Hello, this is a new file create using python ide. n&#39;, &#39;This the second line of the file. n&#39;] . myfile.close() . Writing a file - Understanding the mode . While opening the file, we can open it with different modes . &#39;r&#39; default to read the file | &#39;w+&#39; read and write the file.(Overwrites the existing file) | &#39;wb+&#39; read and write as binary (used in case of pdf) | . myfile = open(&quot;test.txt&quot;, mode= &#39;w+&#39;) myfile.write(&quot;This is an additional file&quot;) . 26 . myfile.seek(0) myfile.readlines() . [&#39;This is an additional file&#39;] . Hence the existing data is deleted and the new data is overwrite. . myfile.close() . Appending a file . Passing the argument &#39;a&#39; opens the file and puts the pointer at the end, so anything written is appended. . myfile = open(&quot;test.txt&quot;, &#39;a+&#39;) myfile.write(&quot; nAppending a new line to the existing line&quot;) myfile.seek(0) print(myfile.read()) . This is an additional file Appending a new line to the existing line Appending a new line to the existing line Appending a new line to the existing line Appending a new line to the existing line . myfile.close() . Aliases and context managers . You can assign temporary variable names as aliases, and manage the opening and closing of files automatically using a context manager: . with open(&#39;test.txt&#39;,&#39;r&#39;) as txt: first_line = txt.readlines()[0] print(first_line) . This is an additional file . By using this method, the file is opened, read and closed by context mananger automatically after doing the specified operation. . first_line . &#39;This is an additional file n&#39; . txt.read() . ValueError Traceback (most recent call last) C: Users VICKY~1.CRA AppData Local Temp/ipykernel_9856/1416744708.py in &lt;module&gt; -&gt; 1 txt.read() ValueError: I/O operation on closed file. . Hence the extract line remain in the object but the file is closed by the context manager. . Iterating through a file . with open(&#39;test.txt&#39;, &#39;r&#39;) as txt: for line in txt: print(line , end=&#39;$$$$&#39;) . This is an additional file $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line $$$$Appending a new line to the existing line$$$$ .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/readlines()/textfiles/nlp-chapter-1/2022/06/28/1-2-Working-with-text-files.html",
            "relUrl": "/readlines()/textfiles/nlp-chapter-1/2022/06/28/1-2-Working-with-text-files.html",
            "date": " • Jun 28, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Coe_policing_equity",
            "content": "Libraries needed . library(tidyverse) . ## Warning: package &#39;tidyverse&#39; was built under R version 4.1.3 ## -- Attaching packages tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.5 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.2 v forcats 0.5.1 ## Warning: package &#39;readr&#39; was built under R version 4.1.3 ## -- Conflicts tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() . library(lubridate) . ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union . Reading the data . coe_df &lt;- read.csv(&quot;37-00049_UOF-P_2016_prepped.csv&quot;, header = T, stringsAsFactors = F) head(coe_df) . ## INCIDENT_DATE INCIDENT_TIME UOF_NUMBER OFFICER_ID OFFICER_GENDER ## 1 OCCURRED_D OCCURRED_T UOFNum CURRENT_BA OffSex ## 2 9/3/16 4:14:00 AM 37702 10810 Male ## 3 3/22/16 11:00:00 PM 33413 7706 Male ## 4 5/22/16 1:29:00 PM 34567 11014 Male ## 5 1/10/16 8:55:00 PM 31460 6692 Male ## 6 11/8/16 2:30:00 AM 37879, 37898 9844 Male ## OFFICER_RACE OFFICER_HIRE_DATE OFFICER_YEARS_ON_FORCE OFFICER_INJURY ## 1 OffRace HIRE_DT INCIDENT_DATE_LESS_ OFF_INJURE ## 2 Black 5/7/14 2 No ## 3 White 1/8/99 17 Yes ## 4 Black 5/20/15 1 No ## 5 Black 7/29/91 24 No ## 6 White 10/4/09 7 No ## OFFICER_INJURY_TYPE OFFICER_HOSPITALIZATION SUBJECT_ID SUBJECT_RACE ## 1 OFF_INJURE_DESC OFF_HOSPIT CitNum CitRace ## 2 No injuries noted or visible No 46424 Black ## 3 Sprain/Strain Yes 44324 Hispanic ## 4 No injuries noted or visible No 45126 Hispanic ## 5 No injuries noted or visible No 43150 Hispanic ## 6 No injuries noted or visible No 47307 Black ## SUBJECT_GENDER SUBJECT_INJURY SUBJECT_INJURY_TYPE ## 1 CitSex CIT_INJURE SUBJ_INJURE_DESC ## 2 Female Yes Non-Visible Injury/Pain ## 3 Male No No injuries noted or visible ## 4 Male No No injuries noted or visible ## 5 Male Yes Laceration/Cut ## 6 Male No No injuries noted or visible ## SUBJECT_WAS_ARRESTED SUBJECT_DESCRIPTION SUBJECT_OFFENSE ## 1 CIT_ARREST CIT_INFL_A CitChargeT ## 2 Yes Mentally unstable APOWW ## 3 Yes Mentally unstable APOWW ## 4 Yes Unknown APOWW ## 5 Yes FD-Unknown if Armed Evading Arrest ## 6 Yes Unknown Other Misdemeanor Arrest ## REPORTING_AREA BEAT SECTOR DIVISION LOCATION_DISTRICT STREET_NUMBER ## 1 RA BEAT SECTOR DIVISION DIST_NAME STREET_N ## 2 2062 134 130 CENTRAL D14 211 ## 3 1197 237 230 NORTHEAST D9 7647 ## 4 4153 432 430 SOUTHWEST D6 716 ## 5 4523 641 640 NORTH CENTRAL D11 5600 ## 6 2167 346 340 SOUTHEAST D7 4600 ## STREET_NAME STREET_DIRECTION STREET_TYPE ## 1 STREET street_g street_t ## 2 Ervay N St. ## 3 Ferguson NULL Rd. ## 4 bimebella dr NULL Ln. ## 5 LBJ NULL Frwy. ## 6 Malcolm X S Blvd. ## LOCATION_FULL_STREET_ADDRESS_OR_INTERSECTION LOCATION_CITY LOCATION_STATE ## 1 Street Address City State ## 2 211 N ERVAY ST Dallas TX ## 3 7647 FERGUSON RD Dallas TX ## 4 716 BIMEBELLA LN Dallas TX ## 5 5600 L B J FWY Dallas TX ## 6 4600 S MALCOLM X BLVD Dallas TX ## LOCATION_LATITUDE LOCATION_LONGITUDE INCIDENT_REASON REASON_FOR_FORCE ## 1 Latitude Longitude SERVICE_TY UOF_REASON ## 2 32.782205 -96.797461 Arrest Arrest ## 3 32.798978 -96.717493 Arrest Arrest ## 4 32.73971 -96.92519 Arrest Arrest ## 5 Arrest Arrest ## 6 Arrest Arrest ## TYPE_OF_FORCE_USED1 TYPE_OF_FORCE_USED2 TYPE_OF_FORCE_USED3 ## 1 ForceType1 ForceType2 ForceType3 ## 2 Hand/Arm/Elbow Strike ## 3 Joint Locks ## 4 Take Down - Group ## 5 K-9 Deployment ## 6 Verbal Command Take Down - Arm ## TYPE_OF_FORCE_USED4 TYPE_OF_FORCE_USED5 TYPE_OF_FORCE_USED6 ## 1 ForceType4 ForceType5 ForceType6 ## 2 ## 3 ## 4 ## 5 ## 6 ## TYPE_OF_FORCE_USED7 TYPE_OF_FORCE_USED8 TYPE_OF_FORCE_USED9 ## 1 ForceType7 ForceType8 ForceType9 ## 2 ## 3 ## 4 ## 5 ## 6 ## TYPE_OF_FORCE_USED10 NUMBER_EC_CYCLES FORCE_EFFECTIVE ## 1 ForceType10 Cycles_Num ForceEffec ## 2 NULL Yes ## 3 NULL Yes ## 4 NULL Yes ## 5 NULL Yes ## 6 NULL No, Yes . ### Removing the first row coe_df &lt;- coe_df[-1,] . ###Changing the format of variables . coe_df$INCIDENT_DATE &lt;- as.Date(coe_df$INCIDENT_DATE, &quot;%m/%d/%y&quot;) coe_df$INCIDENT_TIME &lt;- parse_date_time(coe_df$INCIDENT_TIME, &quot;%I:%M:%S %p&quot;) . ## Warning: 10 failed to parse. . Creating two additional variables. . ### Creating month varible coe_df$month &lt;- month(coe_df$INCIDENT_DATE, label = T, abbr = T) ### Creating time of day variable coe_df$timeofday &lt;- cut(hour(coe_df$INCIDENT_TIME),breaks = c(-Inf,6,12,4,9,23), labels = c(&quot;Night&quot;, &quot;Morning&quot;,&quot;Afternoon&quot;,&quot;Evening&quot;, &quot;Night&quot;)) . 1. Have the number of policing incidents remained constant through the year? . coe_df %&gt;% group_by(month)%&gt;% summarise(incident_count = n())-&gt; incidentsbyyear plot1 &lt;- ggplot(data = incidentsbyyear, aes(x = month, y= incident_count)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;steelblue&quot;) + labs(title= &quot;Number of incidents by month&quot;, x = &quot;Months&quot;, y = &quot;Number of incidents&quot;) plot1 . . We see that number of policing incidents have been decreasing over the year. . 2. At what time of the day are the incident most frequent? . coe_df %&gt;% group_by(timeofday)%&gt;% summarise(incident_count = n())-&gt; incidentsbytimeofday plot2 &lt;- ggplot(data = incidentsbytimeofday, aes(x = timeofday, y= incident_count)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;steelblue&quot;) + labs(title= &quot;Number of incidents by Time of day&quot;, x = &quot;Time of day&quot;, y = &quot;Number of incidents&quot;) plot2 . . We see that majority of the incidents occur at night, that is between 9 PM to 6 AM in the morning. . 3. Does the distribution of the policing incidents and the gender and race of the officer have any relation. . coe_df %&gt;% group_by(OFFICER_GENDER, OFFICER_RACE)%&gt;% summarise(incident_count = n())-&gt; incidentsbyofficer . ## `summarise()` has grouped output by &#39;OFFICER_GENDER&#39;. You can override using the ## `.groups` argument. . plot3 &lt;- ggplot(data = incidentsbyofficer, aes(x = OFFICER_RACE, y= incident_count, fill = OFFICER_GENDER )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Number of incidents by Race of the officer&quot;, x = &quot;Race of the officer&quot;, y = &quot;Number of incidents&quot;) plot3 . . We see that most of the officers involved in the reported incidence are White. Also we find that male officer are in majority compared to female officers. We also have Hispanic and Black officer with a significant number of reported incidences. . 4. Does the gender of the officer and the time of day occurence have any relation. . coe_df %&gt;% filter(timeofday!=&quot;NA&quot;)%&gt;% group_by(OFFICER_GENDER, timeofday)%&gt;% summarise(incident_count = n())-&gt; incidentsbyoffday . ## `summarise()` has grouped output by &#39;OFFICER_GENDER&#39;. You can override using the ## `.groups` argument. . plot4 &lt;- ggplot(data = incidentsbyoffday, aes(x = timeofday, y= incident_count, fill = OFFICER_GENDER )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Number of incidents by Time of day&quot;, x = &quot;Time of day&quot;, y = &quot;Number of incidents&quot;) plot4 . . We see that both the gender the reported incidents are majorly in the night. . 5. Does the Race of the officer has any relationship of getting injuried during the policing incident . coe_df %&gt;% filter(timeofday!=&quot;NA&quot;)%&gt;% group_by(OFFICER_RACE, OFFICER_INJURY)%&gt;% summarise(incident_count = n())-&gt; incidentsbyinjury . ## `summarise()` has grouped output by &#39;OFFICER_RACE&#39;. You can override using the ## `.groups` argument. . plot5 &lt;- ggplot(data = incidentsbyinjury, aes(x = OFFICER_RACE, y= incident_count, fill = OFFICER_INJURY )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Injury to officer and their Race&quot;, x = &quot;Race of the officer&quot;, y = &quot;Number of incidents&quot;) plot5 . . We see that majorly officers and not been injured in the incident. Among those who were injuried we see that most of them are White. . Also a small percentage of Hispanic and Black has been reported to have injuried. . 6. Does the race of subject and the time of day of the incident has any relationship. . coe_df %&gt;% filter(timeofday!=&quot;NA&quot;,SUBJECT_RACE!=&quot;NULL&quot; )%&gt;% group_by(SUBJECT_RACE, timeofday)%&gt;% summarise(incident_count = n())%&gt;%filter(incident_count&gt;20)-&gt; incidentsbysubday . ## `summarise()` has grouped output by &#39;SUBJECT_RACE&#39;. You can override using the ## `.groups` argument. . plot6 &lt;- ggplot(data = incidentsbysubday, aes(x = SUBJECT_RACE, y= incident_count, fill = timeofday )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Race of the subject and time of day&quot;, x = &quot;Race of subject&quot;, y = &quot;Number of incidents&quot;)+ coord_flip() plot6 . . We see that Black subject are mostly in incidences reported in the night or in the evening. There are some case reported the afternoon, but very few in the morning. . Both White and Hispanic are active majorly in the night or evening. . But Hispanic are also active in the morning but White are active in the afternoon. . 7. Does the race of subject and the division of the reported incident has any relationship. . coe_df %&gt;% filter(SUBJECT_RACE!=&quot;NULL&quot; )%&gt;% group_by(SUBJECT_RACE, DIVISION)%&gt;% summarise(incident_count = n())%&gt;%filter(incident_count&gt;20)-&gt; incidentsbydivision . ## `summarise()` has grouped output by &#39;SUBJECT_RACE&#39;. You can override using the ## `.groups` argument. . plot7 &lt;- ggplot(data = incidentsbydivision, aes(x = DIVISION, y= incident_count, fill = SUBJECT_RACE )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Race of the subject and Division&quot;, x = &quot;Division&quot;, y = &quot;Number of incidents&quot;)+ coord_flip() plot7 . . In the South East, South Central, North East and Central division majority of the incidents have Black subjects. . In South West, majority of the incidents involve Hispanic and Blacks. . In South Central is mostly black subjects. However in North Central and Central there are a significant number of white subjects. . 8. Does the race of subject and the arrest has any relationship . coe_df %&gt;% filter(SUBJECT_RACE!=&quot;NULL&quot; )%&gt;% group_by(SUBJECT_RACE, SUBJECT_WAS_ARRESTED)%&gt;% summarise(incident_count = n())%&gt;%filter(incident_count&gt;10)-&gt; incidentsbyarrest . ## `summarise()` has grouped output by &#39;SUBJECT_RACE&#39;. You can override using the ## `.groups` argument. . plot8 &lt;- ggplot(data = incidentsbyarrest, aes(x = SUBJECT_RACE, y= incident_count, fill = SUBJECT_WAS_ARRESTED )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Race of the subject and Arrest Status&quot;, x = &quot;Race of the subject&quot;, y = &quot;Number of incidents&quot;) plot8 . . We see that most the subject irrespective of the race have been arrested. . 9. Does the race of officer and the race of the subject has any evident relationship. . coe_df %&gt;% filter(SUBJECT_RACE!=&quot;NULL&quot;, OFFICER_RACE!=&quot;NULL&quot;)%&gt;% group_by(SUBJECT_RACE, OFFICER_RACE)%&gt;% summarise(incident_count = n())%&gt;%filter(incident_count&gt;10)-&gt; incidentsbyrace . ## `summarise()` has grouped output by &#39;SUBJECT_RACE&#39;. You can override using the ## `.groups` argument. . plot9 &lt;- ggplot(data = incidentsbyrace, aes(x = OFFICER_RACE, y= incident_count, fill = SUBJECT_RACE )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Race of subject vs Race of officer&quot;, x = &quot;Race of the Officer&quot;, y = &quot;Number of incidents&quot;) plot9 . . Irrespective of the race of the officer, most of the incidents involve black subject followed by Hispanic and White. . 10. Does the race of subject and the UOF has any relationship. . coe_df %&gt;% filter(SUBJECT_RACE!=&quot;NULL&quot;, OFFICER_RACE!=&quot;NULL&quot;)%&gt;% group_by(SUBJECT_RACE, REASON_FOR_FORCE)%&gt;% summarise(incident_count = n())%&gt;%filter(incident_count&gt;10)-&gt; incidentsbyUOF . ## `summarise()` has grouped output by &#39;SUBJECT_RACE&#39;. You can override using the ## `.groups` argument. . plot10 &lt;- ggplot(data = incidentsbyUOF, aes(x = SUBJECT_RACE, y= incident_count, fill = REASON_FOR_FORCE )) + geom_bar(stat = &quot;identity&quot;) + labs(title= &quot;Race of subject and reason for use of force&quot;, x = &quot;Race of the Subject&quot;, y = &quot;Number of incidents&quot;) plot10 . . We see that across race of the subject, the chief reason for the use of force is Arrest, Active Aggression and Danger to self or others. . Observations . We see that the number of of policing incidents have been steadily decreasing over the year. They reached a peak in March and they went on decreasing to a reach a minimum in December. . | On looking at the time of reported incidents we see that most of them are reported in the night that is from 9.00 PM to 6.00 AM in the morning. . | From the distribution of the incidents with respect to gender and race of the officer we see that most of the male officer are in majority. . | Irrespective of the gender, White officer has most of the incident reported followed by Hispanic and Black officers. . | Across gender, most of the incidents have been reported at night. . | Looking at the injured officers, we see that in most of the incidents the officers are not injured. However, among the injured officers, White officer have been injured the most. . | Looking at the distribution of incidents with regard to the race of the subject and the time of day, we see that Black subjects are mostly in incidences reported in the night or in the evening. There are some case reported the afternoon, but very few in the morning. Both White and Hispanic are active majorly in the night or evening. But Hispanic are also active in the morning but White are active in the afternoon. . | On analyzing the incidents by Division and race of the subject, we find that in the Southeast, South Central, North East and Central division majority of the incidents have Black subjects. In South West, majority of the incidents involve Hispanic and Blacks. In South Central is mostly black subjects. However in North Central and Central there are a significant number of white subjects. . | Also in the reported incidents, the subjects have been majorly arrested irrespective of the the race. Also irrespective of the race of the officer, the subjects involved in the incident are black. . | Looking at the reason for the use of force we find that Arrest, Active Aggression and Danger to self or others are majorly citied reasons across the race of the subjects. . | .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/2022/06/26/COE_policing_equity.html",
            "relUrl": "/2022/06/26/COE_policing_equity.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "1.1 Learning to use F string literal",
            "content": "Topics covered . F string formating in printing text | Alignment, padding Fstring | Format dates in Printing | . F string basics . person = &quot;Richard&quot; . print(f&quot;The name of the boy is {person}&quot;) . The name of the boy is Richard . Using a dictionary with f string . d = {&#39;Roll no&#39; : 12 , &#39;Subject&#39;: &quot;English&quot;} . print(f&quot;The student with roll no {d[&#39;Roll no&#39;]}, got highest marks in {d[&#39;Subject&#39;]}&quot;) . The student with roll no 12, got highest marks in English . Using a list . l = [&quot;mango&quot;, &quot;orange&quot;,&quot;banana&quot;] . print(f&quot;The fruit that I enjoy the most is {l[0]} and {l[1]}&quot;) . The fruit that I enjoy the most is mango and orange . Minimum Widths, Alignment and Padding . You can pass arguments inside a nested set of curly braces to set a minimum width for the field, the alignment and even padding characters. . library = [(&#39;Author&#39;, &#39;Topic&#39;, &#39;Pages&#39;), (&#39;Twain&#39;, &#39;Rafting&#39;, 601), (&#39;Feynman&#39;, &#39;Physics&#39;, 95), (&#39;Hamilton&#39;, &#39;Mythology&#39;, 144)] . Tuple unpacking . for author, topic, page in library: print(f&quot;{author}, {topic},{page}&quot;) . Author, Topic,Pages Twain, Rafting,601 Feynman, Physics,95 Hamilton, Mythology,144 . aligning the text . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{8}}{page:{7}}&quot;) . Author Topic Pages Twain Rafting 601 Feynman Physics 95 Hamilton Mythology 144 . Here the first three lines align, except Pages follows a default left-alignment while numbers are right-aligned. Also, the fourth line&#39;s page number is pushed to the right as Mythology exceeds the minimum field width of 8. When setting minimum field widths make sure to take the longest item into account. . To set the alignment, use the character &lt; for left-align, ^ for center, &gt; for right. To set padding, precede the alignment character with the padding character (- and . are common choices). . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{10}}{page:&gt;{7}}&quot;) . Author Topic Pages Twain Rafting 601 Feynman Physics 95 Hamilton Mythology 144 . for author, topic, page in library: print(f&quot;{author:{10}} {topic:{10}}{page:.&gt;{7}}&quot;) . Author Topic ..Pages Twain Rafting ....601 Feynman Physics .....95 Hamilton Mythology ....144 . . from datetime import datetime . today = datetime(year=2022, month =1, day = 27) . print(f&#39;{today:%B,%d, %Y}&#39;) . January,27, 2018 .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/nlp/nlp-chapter-1/2022/06/25/1-1-Text-Basics-using-Fstring-literal.html",
            "relUrl": "/nlp/nlp-chapter-1/2022/06/25/1-1-Text-Basics-using-Fstring-literal.html",
            "date": " • Jun 25, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://vicky-crasto.github.io/Learn-Project-Language/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vicky-crasto.github.io/Learn-Project-Language/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vicky-crasto.github.io/Learn-Project-Language/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}